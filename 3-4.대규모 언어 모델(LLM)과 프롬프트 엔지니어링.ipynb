{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0f2da723",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 0
        }
      },
      "source": [
        "# 대규모 언어 모델과 프롬프트 엔지니어링"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f54990ff",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 1
        }
      },
      "source": [
        "## 1. LLM 시대의 도래\n",
        "\n",
        "### 1.1 인공지능의 최근 변화와 LLM의 등장\n",
        "\n",
        "- 인공지능(AI)은 오랫동안 연구되어 왔지만, 최근 몇 년 사이에 **눈에 띄는 도약**을 이루었습니다.  \n",
        "- 그 중심에는 **대규모 언어 모델(LLM, Large Language Model)** 이 있습니다.  \n",
        "- 기존 AI는 특정 작업(예: 사진 분류, 번역 등)에 특화되어 있었던 반면, LLM은 **텍스트 이해와 생성**이라는 범용 능력을 바탕으로,  \n",
        "  여러 분야에 유연하게 적용될 수 있습니다는 점에서 혁신적입니다.  \n",
        "\n",
        "즉, LLM은 단순한 \"도구\"가 아니라, 다양한 문제 해결의 **플랫폼**이 되고 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d5f1925",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 2
        }
      },
      "source": [
        "### 1.2 LLM과 AI 에이전트(Agent)\n",
        "\n",
        "최근 많이 언급되는 **AI 에이전트**는 특정 목표를 달성하기 위해 **스스로 계획을 세우고, 외부 도구를 활용하며, 지속적으로 상호작용하는 AI 시스템**을 말합니다.  \n",
        "이러한 에이전트의 뇌와 같은 역할을 하는 것이 바로 **LLM**입니다.  \n",
        "\n",
        "예시:  \n",
        "- 일정 관리 AI → LLM이 사용자 요청을 이해 → 캘린더 API를 호출해 일정 등록  \n",
        "- 고객 상담 AI → 고객의 긴 대화 내용을 이해 → 적절한 답변 생성 및 FAQ 검색  \n",
        "\n",
        "LLM이 없으면 에이전트가 스스로 생각하고 판단하기 어렵습니다.\n",
        "즉, LLM은 차세대 AI 에이전트의 두뇌이자 핵심 동력이라고 할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "541fc204",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 3
        }
      },
      "source": [
        "### 1.3 산업·사회 전반에 끼친 영향\n",
        "\n",
        "LLM은 이미 다양한 분야에 깊숙이 들어와 있습니다.\n",
        "\n",
        "<div style=\"display: flex; align-items: flex-start;\">\n",
        "  <img src=\"image/chatgpt.png\" width=\"400\" style=\"margin-right: 20px;\"/>\n",
        "\n",
        "  <div>\n",
        "\n",
        "- **교육**: 자동 채점, 튜터링, 학습 콘텐츠 생성  \n",
        "- **비즈니스**: 이메일 초안 작성, 보고서 요약, 회의록 자동화  \n",
        "- **헬스케어**: 의학 논문 요약, 환자 기록 정리, 의료 상담 챗봇  \n",
        "- **법률**: 판례 검색, 계약서 초안 작성, 법률 자문 보조  \n",
        "- **연구·개발**: 코드 생성, 실험 보고서 작성, 논문 요약  \n",
        "- **창작 분야**: 시나리오 작성, 마케팅 카피, 음악/그림 창작  \n",
        "- **고객 서비스**: 24시간 챗봇, 다국어 고객 응대  \n",
        "\n",
        "  </div>\n",
        "</div>\n",
        "\n",
        "이처럼 LLM은 **효율성을 극대화**하고, 인간의 창의적 활동을 보조하는 역할을 하며 사회 전반에 영향을 주고 있습니다.   \n",
        "LLM을 제대로 이해하고 다룰 수 있게 되면, 단순한 자동화 그 이상으로 **더 빠르고, 더 창의적이며, 더 생산적인 방식으로 문제를 해결할 수 있는 강력한 도구를 얻게 될 것입니다.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e890da9a",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 4
        }
      },
      "source": [
        "### 1.4 수업 목표\n",
        "\n",
        "1. **LLM의 기본 개념 이해**  \n",
        "   - LLM(대규모 언어모델)의 정의와 발전 과정을 이해하고, 왜 인공지능 발전의 핵심 기술로 주목받는지 설명할 수 있음  \n",
        "\n",
        "2. **LLM의 작동 원리 이해**  \n",
        "   - 단순한 \"문자 생성기\"가 아닌, 방대한 데이터와 확률적 언어 패턴을 학습하여 문맥을 이해하고 추론하는 원리를 파악할 수 있음  \n",
        "   \n",
        "   - 인간 피드백을 활용한 강화학습(**RLHF: Reinforcement Learning from Human Feedback**)의 기본 개념과 적용 목적을 이해할 수 있음  \n",
        "\n",
        "3. **LLM 활용 능력 습득**  \n",
        "\n",
        "    - **프롬프트 엔지니어링** 기법을 익혀, 모델의 출력을 원하는 방향으로 조정하고 제어할 수 있음  \n",
        "   \n",
        "    - **할루시네이션(Hallucination)** 현상의 원인을 이해하고, 이를 방지하거나 완화하기 위한 실무적 테크닉을 습득함  \n",
        "   \n",
        "    - **최신 LLM 모델의 동향 및 특징**을 살펴보고, 각 모델의 강점과 활용 분야를 비교·이해함  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c15b3c16",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 5
        }
      },
      "source": [
        "### 사전 준비 1: Gemma 3 모델 사용 설정\n",
        "\n",
        "대형 언어모델(LLM)은 수십~수백 GB의 파라미터를 갖고 있으며, 이를 빠르게 추론·학습하려면  \n",
        "A100/H100 같은 **고가의 GPU와 복잡한 서버 환경**이 필요합니다.  \n",
        "따라서 일반 개인의 로컬 환경에서 GPT-4 수준의 대규모 모델을 직접 실행하기는 사실상 어렵습니다.\n",
        "\n",
        "이번 실습의 목적은 \"대형 LLM을 직접 띄우기 어려운 이유\"를 이해하면서도,  \n",
        "**로컬 환경에서 모델을 직접 불러와 동작시키는 경험 자체는 유지하는 것**입니다.  \n",
        "이를 위해 가볍고 빠르며, 저사양 GPU에서도 원활히 실행되는 **`gemma-3-270m`** 모델을 사용합니다.\n",
        "\n",
        "즉,  \n",
        "- **Gemma 3 → 로컬 환경에서의 LLM 실습 및 미세조정(Fine-tuning) 체험용**  \n",
        "- **GPT API → 실제 서비스/고성능 모델 사용 시의 클라우드형 접근 방식**  \n",
        "\n",
        "이 두 방식을 비교하면서 학습하게 됩니다.\n",
        "\n",
        "아래는 두 방식의 차이를 쉽게 이해할 수 있는 비교표입니다.\n",
        "\n",
        "| 방식 | 장점 | 단점 | 활용 목적 |\n",
        "|------|------|-------|------------|\n",
        "| **로컬 실행 모델 (Gemma 3)** | 빠른 개발 실험, 비용 없음, 인터넷 불필요, 모델 구조·파이프라인을 직접 제어 가능 | 성능이 낮음, GPU 메모리 영향 큼 | 실습·연구·모델 원리 이해 |\n",
        "| **클라우드 API LLM** | 매우 높은 성능과 품질, 별도 학습·세팅 불필요, 바로 사용 가능 | 사용량 과금, API Key 관리 필요, 인터넷 필수 | 실제 서비스 개발 및 고성능 AI 기능 구현 |\n",
        "\n",
        "이 실습에서는 **로컬 LLM(Gemma 3)을 통한 모델 동작 이해**가 핵심이며,  \n",
        "이후 이어지는 API 파트에서는 **고사양 LLM을 원격으로 사용하는 방법**을 배우게 됩니다.\n",
        "\n",
        "\n",
        "####  Hugging Face에서 Gemma 3 접근 신청  \n",
        "1. [https://huggingface.co/google/gemma-3-270m-it](https://huggingface.co/google/gemma-3-270m-it) 페이지로 이동  \n",
        "2. **\"Access request\"** 버튼을 클릭하고, 이용 약관에 동의  \n",
        "3. 승인 후 계정의 Access Token을 발급받습니다.\n",
        "\n",
        "\n",
        "####  `.env` 파일에 환경변수 등록  \n",
        "프로젝트 루트 디렉토리에 `.env` 파일을 생성하고 아래 내용을 추가합니다.\n",
        "\n",
        "```bash\n",
        "HF_TOKEN=your_huggingface_access_token\n",
        "```\n",
        "\n",
        ">  `your_huggingface_access_token` 부분을 실제 본인의 토큰으로 교체하세요.  \n",
        "> 이 토큰은 모델을 불러올 때 Hugging Face API 인증에 사용됩니다.\n",
        "\n",
        "####  코드 실행 전 환경변수 로드  \n",
        "Python 코드 상단이나 Jupyter Notebook에서 다음 코드를 추가하면 됩니다.\n",
        "\n",
        "```python\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "load_dotenv()  # .env 파일 불러오기\n",
        "os.environ[\"HF_TOKEN\"]\n",
        "```\n",
        "\n",
        "이 과정을 마친 후, 아래 예제 코드를 실행하면 Gemma 3 모델을 정상적으로 불러올 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "8e374fff",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 6
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "인공지능 시대에 학교에서 어떤 과목을 배우면 좋을까? \n",
            "\n",
            "인공지능 시대에 교육 시스템의 변화와 함께 사회적 영향과 다양한 과목 선택의 폭이 넓어지고 있습니다. 학교 교육 시스템은 학생들의 학습 능력 향상, 학업 성취도 증진, 그리고\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from dotenv import load_dotenv\n",
        "import torch\n",
        "load_dotenv()\n",
        "\n",
        "# 모델과 토크나이저 불러오기\n",
        "model_name = \"google/gemma-3-270m-it\"   # instruction 튜닝 버전 사용\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# 프롬프트 입력\n",
        "prompt = \"인공지능 시대에 학교에서 어떤 과목을 배우면 좋을까?\"\n",
        "\n",
        "# 토큰화 및 모델 입력\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "outputs = model.generate(**inputs, max_new_tokens=50)\n",
        "\n",
        "# 결과 출력\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c91bab33",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 7
        }
      },
      "source": [
        "### 사전 준비 2: OpenAI API 키 설정 (.env 파일, 보안 주의)\n",
        "\n",
        "로컬에서는 Gemma 3처럼 가벼운 모델을 직접 실행할 수 있지만,  **고성능 GPT-4/5 등 대형 모델**을 로컬에서 구동하기에는  \n",
        "하드웨어 요구사항(A100/H100급 GPU)과 운영 비용이 매우 큽니다.\n",
        "\n",
        "실제 서비스 개발에서는 높은 정확도, 복잡한 reasoning, 안정적인 추론 품질을 위해  \n",
        "**클라우드에서 제공하는 고사양 LLM을 API 방식으로 호출**하는 방식을 주로 사용합니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a86f6d44",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 8
        }
      },
      "source": [
        "#### API란?\n",
        "\n",
        "API는 하나의 프로그램이 외부 서버나 다른 소프트웨어와  **데이터를 주고받기 위한 통신 규칙(인터페이스)** 입니다.\n",
        "\n",
        "LLM API를 사용할 때는 다음과 같은 흐름으로 동작합니다:\n",
        "\n",
        "1. 우리가 작성한 프로그램이 OpenAI 서버에 요청(request) 을 보냄  \n",
        "2. OpenAI 서버의 LLM이 요청을 처리  \n",
        "3. 결과를 응답(response) 으로 우리 프로그램에 반환  \n",
        "\n",
        "즉, **\"고성능 LLM 모델을 직접 설치하지 않아도, 인터넷을 통해 즉시 사용할 수 있게 해주는 방식\"** 이 API입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2346a2e",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 9
        }
      },
      "source": [
        "#### API Key란?\n",
        "\n",
        "API 서버(OpenAI)가 누가 요청을 보냈는지, 얼마만큼 사용했는지, 어떤 요금이 발생했는지  \n",
        "를 추적하기 위해 발급하는 **개인 전용 인증 토큰**입니다.\n",
        "\n",
        "- API Key가 있어야 GPT API 호출 가능  \n",
        "- 사용량·요금이 이 Key 기준으로 계산  \n",
        "- 외부에 유출될 경우 타인이 마음대로 사용하여 **과금 사고 발생 위험**\n",
        "\n",
        "따라서 반드시 안전하게 관리해야 합니다.\n",
        "\n",
        "\n",
        "#### API Key 발급 절차\n",
        "\n",
        "1. https://platform.openai.com/ 접속  \n",
        "2. 로그인 또는 계정 생성  \n",
        "3. 우측 상단 프로필 아이콘 → **`View API Keys`**  \n",
        "4. 결제 수단 등록(신용카드 등)  \n",
        "5. **`+ Create new secret key`** 클릭  \n",
        "6. 생성된 Key를 **즉시 복사해 안전한 곳에 보관**  \n",
        "   - ※ 발급 직후 한 번만 표시됨\n",
        "> ※ 생성된 API Key는 한 번만 보이므로 반드시 백업해야 합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3f537d4",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 10
        }
      },
      "source": [
        "#### .env 파일에 저장하기\n",
        "\n",
        "프로젝트 루트에 `.env` 파일을 생성하고 다음 형식으로 저장합니다:\n",
        "\n",
        "```bash\n",
        "OPENAI_API_KEY=sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
        "```\n",
        "\n",
        "이 값은 외부에 공유하거나 깃허브에 업로드하면 절대 안 됩니다.  \n",
        "아래 코드는 .env 파일에 저장된 Key를 안전하게 불러오는 방식입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23c72aa0",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 11
        }
      },
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "load_dotenv()  # .env 파일 읽기\n",
        "#print(os.getenv(\"OPENAI_API_KEY\")) # 환경 변수에 api 키가 잘 등록되었는지 ]확인"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6dc04eb0",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 12
        }
      },
      "source": [
        "#### 보안 주의사항\n",
        "\n",
        ".env 파일은 절대 GitHub, Notion, Google Drive 등의 공개 저장소에 업로드 금지\n",
        "\n",
        "실습 중 노출되었다면 즉시 해당 Key 삭제 후 재발급 API Key를 직접 코드에 하드코딩하지 말 것\n",
        "\n",
        "팀 작업 시에도 각자 Key 발급을 원칙으로 함 (공유 금지)\n",
        "\n",
        "이 과정을 마치면, 고성능 LLM을 API 방식으로 호출할 준비가 완료됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ed8e894",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 13
        }
      },
      "source": [
        "#### OpenAI API로 간단한 ChatGPT 호출 테스트\n",
        "\n",
        "API Key 설정이 완료되었다면, 실제로 OpenAI API에 요청을 보내  \n",
        "모델이 정상적으로 동작하는지 확인할 수 있습니다.\n",
        "\n",
        "아래 예시는 **`gpt-4o-mini` 모델로 간단한 채팅 요청을 보내는 기본 코드**입니다.  \n",
        "(원하는 경우 `model=` 부분을 다른 최신 모델 이름으로 교체하면 됩니다.)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed849411",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 14
        }
      },
      "source": [
        "프롬프트란, 모델에게 주어지는 **질문, 명령 또는 문장의 입력 형식**을 말하며,  \n",
        "이 프롬프트와 함께 `temperature`, `max_tokens`, `top_p` 등의 하이퍼파라미터를 조정하면  \n",
        "**모델이 생성하는 응답의 창의성, 길이, 다양성 등을 직접 제어**할 수 있습니다.\n",
        "\n",
        "아래는 OpenAI API를 사용할 때 자주 설정하게 되는 핵심 매개변수들을 정리한 표입니다.  \n",
        "각 옵션을 이해하고 적절히 조절하면 모델의 출력 방식과 품질을 더욱 세밀하게 제어할 수 있습니다.\n",
        "\n",
        "| 매개변수 | 설명 |\n",
        "|----------|------|\n",
        "| `model` | 사용할 GPT 모델 이름 (`gpt-3.5-turbo`, `gpt-4`, 등) |\n",
        "| `messages` | 대화 문맥. `system`, `user`, `assistant` 역할로 구성 |\n",
        "| `temperature` | 출력의 무작위성 조절. 높을수록 다양하고 창의적인 답변 (0~ 2) |\n",
        "| `max_tokens` | 생성할 응답의 최대 길이를 지정하는 값. **영문 기준 약 1토큰 ≈ 0.75단어**, **한글 기준 1토큰 ≈ 1~2글자** 정도로 계산됨 |\n",
        "| `top_p` | 확률 누적 기반 토큰 필터링. 창의성 제어 기법 중 하나 |\n",
        "| `frequency_penalty` | 반복 방지를 위한 페널티 (0 ~ 2) |\n",
        "| `presence_penalty` | 새로운 주제 도입을 유도 (0 ~ 2) |\n",
        "| `n` | 응답 개수. 다중 응답 실험 시 유용 |\n",
        "| `stop` | 응답 생성을 중단할 문자열 또는 토큰 지정 |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "7d6e19d9",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 15
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "모델 응답: 저는 OpenAI에서 개발한 AI 언어 모델인 ChatGPT입니다. 자연어 처리를 기반으로 한 프로그램으로, 다양한 주제에 대한 질문에 답변하고, 정보를 제공하며, 대화를 나눌 수 있습니다. \n",
            "\n",
            "### 주요 기능\n",
            "1. **정보 제공**: 과학, 역사, 기술 등 여러 분야에 대한 정보를 제공합니다.\n",
            "2. **질문 및 답변**: 사용자의 질문에 대한 답변을 제공합니다.\n",
            "3. **창작 지원**: 글쓰기, 이야기, 시, 코드 작성 등을 도와줍니다.\n",
            "4. **언어 이해**: 다양한 언어로 대화하고, 쉽게 이해할 수 있는 답변을 제공합니다.\n",
            "\n",
            "### 한계점\n",
            "- 최신 정보에 대한 접근이 제한되어 있으며, 2023년 10월까지의 데이터로 훈련되었습니다.\n",
            "- 개인적인 경험이나 감정이 없으며, 사용자 개인 정보는 수집하지 않습니다.\n",
            "\n",
            "필요한 정보나 질문이 있다면 언제든지 물어보세요!\n"
          ]
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI\n",
        "import os\n",
        "\n",
        "# .env 로드 (.env 파일에 OPENAI_API_KEY가 있어야 합니다)\n",
        "load_dotenv()\n",
        "#api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",  # 사용 모델\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"너에 대해 자세히 소개해줘\"}\n",
        "    ],\n",
        "    max_tokens=500,\n",
        ")\n",
        "\n",
        "print(\"모델 응답:\", response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93283595",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 16
        }
      },
      "source": [
        "## 2. LLM 기본 이해\n",
        "\n",
        "이 장에서는 LLM의 정의, 작동 원리, 그리고 주요 이슈를 살펴봅니다.  \n",
        "LLM을 제대로 활용하기 위해서는 단순히 \"잘 쓴다\"가 아니라, **왜 그런 답이 나오는지 기본 구조와 한계**를 이해하는 것이 필요합니다.   "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b736c4e7",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 17
        }
      },
      "source": [
        "### 2.1 LLM 정의와 특징\n",
        "\n",
        "#### LLM이란?\n",
        "- **LLM (Large Language Model)** 은 방대한 양의 텍스트 데이터를 학습해, 단어와 문장의 **확률적 패턴**을 모델링한 인공지능입니다.  \n",
        "- 수십억 개의 매개변수(parameter)를 가진 대규모 신경망으로, 사람처럼 문장을 이해하고 생성할 수 있습니다.  \n",
        "\n",
        "#### 모델 구조 개요 (Transformer 기반)\n",
        "- 현대 LLM의 뼈대는 <a href=\"https://arxiv.org/abs/1706.03762\">**Transformer 구조(2017)**</a> 입니다.  \n",
        "- Transformer는 **Self-Attention** 메커니즘을 이용해, 문장 안의 단어들 사이 관계를 파악합니다.  \n",
        "\n",
        "예시:  \n",
        "- 문장 `\"고양이가 물을 마신다\"`에서  \n",
        "  - \"고양이\"와 \"마신다\"는 멀리 떨어져 있어도 서로 연결됨  \n",
        "  - Transformer는 이런 관계를 동시에 고려 가능  \n",
        "\n",
        "#### Scaling Law\n",
        "- 참고 논문 : https://arxiv.org/abs/2001.08361\n",
        "- 연구 결과: **모델 크기(파라미터 수), 데이터 양, 계산량**이 늘어날수록 모델 성능이 꾸준히 개선됨  \n",
        "- 이 현상을 **Scaling Law**라고 부릅니다.  \n",
        "- 즉, 모델을 크게 만들면 만들수록(적절한 데이터와 연산 자원이 뒷받침된다면) 성능이 좋아집니다.  \n",
        "\n",
        "이는 최근 수많은 초거대 모델(GPT-4, Gemini, Claude 등)이 등장한 중요한 이유입니다.  \n",
        "\n",
        "\n",
        "#### 주요 특징\n",
        "1. **범용성 (Generality)**  \n",
        "   - 번역, 요약, 질문응답, 코드 작성 등 다양한 작업에 한 모델이 활용됨  \n",
        "\n",
        "2. **확장성 (Scalability)**  \n",
        "   - 데이터와 파라미터가 늘어날수록 성능이 개선되는 구조  \n",
        "\n",
        "3. **전이 학습 (Transfer Learning)**  \n",
        "   - 방대한 데이터로 사전학습 → 이후 특정 작업에 맞게 소량 데이터로 미세 조정"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d7003ae",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 18
        }
      },
      "source": [
        "### 2.2 LLM의 학습 과정\n",
        "\n",
        "#### 토큰화와 확률적 언어 모델링\n",
        "- 텍스트는 먼저 **토큰(token)** 단위로 나뉘어 고유한 숫자(ID) 로 변환됨\n",
        "- 모델은 토큰 시퀀스를 보고 **다음 토큰이 등장할 확률 분포**를 예측함\n",
        "- 가장 높은 확률을 가진 토큰을 이어 붙여 문장을 생성함 \n",
        "\n",
        "#### 사전 학습 (Pretraining)\n",
        "- 대규모 텍스트 코퍼스(위키백과, 책, 웹 문서 등)를 기반으로 **다음 단어 맞추기(next-token prediction)** 방식으로 학습함\n",
        "- 예: `\"나는 밥을 ___\"` → \"먹는다\" 확률이 가장 높은 단어를 예측  \n",
        "\n",
        "| 작업(Task)           | 사전학습 중 학습할 수 있는 자연스러운 예시 문장 |\n",
        "|----------------------|-----------------------------------------|\n",
        "| 문법 (Grammar)       | 그는 어제 {도서관, 공원}에 갔다. |\n",
        "| 어휘 의미 (Lexical Semantic) | 나는 아침에 {커피, 차}를 마셨다. |\n",
        "| 세계 지식 (World Knowledge) | 미국의 수도는 {워싱턴, 뉴욕}입니다. |\n",
        "| 감정 분석 (Sentiment Analysis) | 리뷰: 이 드라마는 {재미있었다, 지루했다}. |\n",
        "| 수학 문제 (Math Question) | 계산 문제: 5 × 6 = {30, 42} |\n",
        "| 번역 (Translation)   | ‘고마워’를 영어로 하면 {thank you, hello}입니다. |\n",
        "\n",
        "\n",
        "#### 미세 조정 (Fine-tuning)\n",
        "- 특정 작업(예: 법률 상담, 의료 상담)에 특화되도록 도메인별 소량 데이터로 추가 학습함\n",
        "- 이를 통해 일반 언어모델이 특정 분야의 문체와 어휘를 학습하여 전문화된 응답을 생성할 수 있음"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb2500ff",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 19
        }
      },
      "source": [
        "#### Instruct 모델\n",
        "- 최근 챗봇들이 사람의 질문에 자연스럽게 대답할 수 있는 이유는 **Instruct 모델** 덕분임  \n",
        "- Instruct 모델은 단순히 언어의 다음 단어를 예측하는 것이 아니라, **사람의 지시(Instruction)** 를 이해하고 따르도록 추가 학습된 형태임  \n",
        "- 특히 **ChatGPT(GPT-3.5)** 는 다음의 **3단계 학습 과정(SFT → RM → RLHF)** 을 통해 인간의 선호도를 학습 과정에 반영함으로써,  \n",
        "GPT-3에서 나타났던 편향된 표현이나 부정확한 응답 문제를 개선함\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34d7333f",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 20
        }
      },
      "source": [
        "**1단계 : 지도 미세조정(Supervised Fine-Tuning, SFT)**  \n",
        "사람이 직접 작성한 \"지시–답변\" 쌍(예: *‘이 문장 요약해줘 → 요약된 문장’*)으로 모델을 훈련시킴.  \n",
        "이를 통해 모델이 지시의 형식과 대화체 문장을 이해하도록 만듦.  \n",
        "\n",
        "예시 :  \n",
        "| 지시(Instruction) | 답변(Response) |\n",
        "|-----------|------|\n",
        "| 인공지능이 의료 분야에서 어떻게 활용되는지 설명해 주세요. | 인공지능은 의료 영상 분석, 질병 예측, 신약 개발 등 다양한 분야에 활용됩니다. 예를 들어, AI는 CT나 MRI 이미지를 분석해 암 조직을 조기에 탐지하고, 환자의 유전자 정보를 기반으로 맞춤형 치료를 지원합니다. |\n",
        "| Python에서 딕셔너리와 리스트의 차이점을 설명해 주세요. | 리스트는 순서가 있는 데이터 구조이며 인덱스로 접근하지만, 딕셔너리는 키(key)-값(value) 쌍으로 데이터를 저장해 키를 통해 빠르게 접근할 수 있습니다. 예: `my_dict[\"name\"] = \"Alice\"` |\n",
        "\n",
        "\n",
        "**실제 학습 방식** :  \n",
        "모델은 \"질문 + 답변\"이 이어진 문장에서 **다음 단어를 예측(next token prediction)** 하는 방식으로 학습함.  \n",
        "\n",
        "예시:  \n",
        "```yaml\n",
        "입력: 인공지능이 의료 분야에서 어떻게 활용되는지 설명해 주세요 인공지능은\n",
        "출력 예측: 의료\n",
        "```\n",
        "\n",
        "SFT는 인간이 원하는 방향으로 모델의 기본 성능을 빠르게 개선할 수 있는 효율적인 방법이지만,\n",
        "데이터셋 구축에 많은 시간과 비용이 들며, 질문당 하나의 정답만 학습하기 때문에\n",
        "답변이 획일화되거나 창의성이 제한될 수 있음"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5faa00b",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 21
        }
      },
      "source": [
        "**2단계 : 보상 모델 학습(Reward Model Training)**  \n",
        "\n",
        "**보상 모델(Reward Model)** 은 인간 평가자의 **선호도(preference)** 를 학습하는 모델로,  \n",
        "  SFT 모델이 생성한 여러 답변 중 **사람이 더 선호하는 답변(A가 B보다 낫다)** 을 구분하도록 학습됨.  \n",
        "\n",
        "동일한 질문에 대해 여러 답변을 생성하고, 사람이 각 답변의 품질을 비교하여  \n",
        "  \"좋은/나쁜 답변\"을 평가한 데이터를 바탕으로 **보상 점수(reward score)** 를 학습함.  \n",
        "\n",
        "이 모델은 이후 강화학습(RLHF) 단계에서 **\"사람이 좋아할 답변\"을 보상으로 판단하는 기준** 역할을 하며,  \n",
        "  인간의 선호도를 **정량화하여 모델이 응답 품질을 스스로 평가할 수 있도록 하는 단계**임.\n",
        "\n",
        "  <img src=\"image/reward_model.png\" width=\"600\">\n",
        "\n",
        "\n",
        "**데이터셋 구성 과정**\n",
        "\n",
        "1. **SFT 모델**이 각 **지시문(프롬프트)** 에 대해 **k개의 후보 답변**을 생성함.  \n",
        "2. **인간 평가자(human rater)** 가 이 k개의 답변을 **전체 순위(ranking)** 로 평가함.  \n",
        "3. 이 순위를 기반으로 가능한 모든 쌍을 조합해 **이진 비교(pairwise comparison)** 데이터를 생성함.  \n",
        "   - 생성 가능한 쌍의 수는 **k(k−1)/2**  \n",
        "   - 상대적 순위가 높은 답변을 **선호답변(preferred)**,  \n",
        "     낮은 답변을 **비선호답변(non-preferred)** 으로 간주함.  \n",
        "4. 최종 데이터셋은 `(프롬프트, 선호답변, 비선호답변)` 형태로 구성됨\n",
        "\n",
        "**특징**\n",
        "\n",
        "- 인간의 선호도를 정량화하여 **미세한 품질 차이까지 학습 가능**  \n",
        "- **사람의 평가 기준을 보상 신호로 변환**해 모델이 스스로 \"좋은 답변\"을 구별하도록 만듦  \n",
        "- 그러나 평가자의 **주관성에 따른 편향(bias)** 이 발생할 수 있으며, **데이터 구축에 많은 시간과 비용이 소요**된다는 한계가 있음"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64eb1c64",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 22
        }
      },
      "source": [
        "**3단계 : 강화학습(Reinforcement Learning from Human Feedback, RLHF)**\n",
        "\n",
        "**강화학습이란?**\n",
        "\n",
        "**강화학습(RL)** 은 행동 → 보상의 반복을 통해 **정책(policy)** 을 개선하는 학습 방식임.  \n",
        "  - *정책(Policy)*: 어떤 입력(지시/문맥)이 주어졌을 때 **어떤 답변을 낼지**를 결정하는 모델의 규칙(분포).  \n",
        "  - *보상(Reward)*: 그 답변이 **얼마나 사람 선호에 맞는지**를 수치로 표현한 값.\n",
        "\n",
        "> 비유: 학생(정책)이 답안을 쓰면, 선생님(보상)이 점수를 매기고, 학생은 점수를 최대화하도록 답안 스타일을 조정한다.\n",
        "\n",
        "\n",
        "**왜 SFT만으로는 부족한가?**\n",
        "- **SFT**는 \"좋은 예시를 모방\"하는 단계라 **한 가지 정답 스타일**에 치우치거나 **일반화/적응력**이 제한될 수 있음.  \n",
        "- **RLHF**는 사람의 선호를 직접 보상으로 삼아 **\"사람이 진짜 좋아하는 방향\"**으로 정책을 **적극적으로 최적화**함.\n",
        "\n",
        "\n",
        "**InstructGPT에서의 RLHF 절차**\n",
        "1. **정책 초기화**: SFT로 만들어둔 모델을 **초기 정책 π**로 사용  \n",
        "2. **답변 샘플링**: 프롬프트를 넣어 **여러 답변을 생성**  \n",
        "3. **보상 계산**: 각 답변에 대해 **보상모델(RM)** 이 **스칼라 점수**를 부여  \n",
        "4. **정책 업데이트(PPO)**: 점수가 높아지도록 **정책 파라미터를 조정**  \n",
        "   - 이때 **KL 패널티(= 참고/레퍼런스 모델과의 거리 제어)** 를 걸어,  \n",
        "     \"점수는 올리되 문체가 이상하게 무너지지 않도록\" **안정성**을 확보  \n",
        "5. 위 과정을 **여러 라운드 반복** → 사람이 선호하는 방향으로 말투·내용·안전성이 **점진적으로 향상**\n",
        "\n",
        "> 핵심: \"RM 점수(사람 선호)를 올려라 + 레퍼런스(SFT)에서 너무 멀어지지 마라(KL)\".\n",
        "\n",
        "\n",
        "**PPO가 하는 일(직관적으로)**\n",
        "- **PPO(Proximal Policy Optimization)** 는 \"좋아진 방향으로 **조금씩** 업데이트\"하여  \n",
        "  **폭주/붕괴 없이 안정적으로** 정책을 개선하는 방법.  \n",
        "- 보상(=RM 점수)과 **KL 패널티**(=레퍼런스 모델과의 차이 비용)를 함께 고려해  \n",
        "  **유용함(선호)과 안전/일관성** 간 **균형**을 맞춤.\n",
        "\n",
        "\n",
        "**작은 예시**\n",
        "- 지시: \"초등학생도 이해할 수 있게 ‘광합성’을 설명해줘.\"  \n",
        "- 정책이 낸 답변 A, B, C 중에서 RM이 **A에 2.8점, B에 1.4점, C에 0.9점**을 줌.  \n",
        "- PPO는 **A 같은 답변이 더 잘 나오도록** 정책을 업데이트하되,  \n",
        "  **SFT 레퍼런스와 너무 다른 말투/형식**으로 변하지 않게 **KL로 제어**함.\n",
        "\n",
        "\n",
        "**실제 학습 레시피(요약)**\n",
        "- **정책 모델**: SFT 완료 모델(초기 상태)  \n",
        "- **레퍼런스 모델**: SFT 고정본(비교용)  \n",
        "- **보상 모델**: 인간 선호로 학습된 RM(스칼라 점수)  \n",
        "- **목표**: `RM 점수 ↑` + `KL(정책‖레퍼런스) ↓`  \n",
        "- **알고리즘**: PPO(클리핑/어드밴티지 추정 등으로 안정성 확보)  \n",
        "- **반복**: 샘플링 → 보상 → 업데이트 → 수렴할 때까지"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca47dfba",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 23
        }
      },
      "source": [
        "**결과와 한계**\n",
        "\n",
        "RLHF를 거친 모델은 단순히 데이터를 모방하는 수준을 넘어, 사람의 지시를 이해하고**, **공손하며**, **도움이 되는 응답**을 생성하도록 발전함.  \n",
        "즉, 언어 예측 중심의 모델이 **\"사람의 의도를 반영해 소통하는 대화형 모델\"** 로 진화한 것임.\n",
        "\n",
        "- **한계(Limitation)**: RLHF의 성능은 **보상모델(RM)** 의 품질과 **인간 평가자의 편향(bias)** 에 민감하게 영향을 받음.  \n",
        "  또한 **KL 제약(KL penalty)** 의 강도가 모델의 창의성과 일관성에 직결되므로,  \n",
        "  - 제약이 너무 강하면 → 답변이 경직되고 창의성이 저하됨  \n",
        "  - 제약이 너무 약하면 → 말투 붕괴나 비일관적 응답이 발생  \n",
        "  따라서 **보상 신호, KL 제약, 샘플링 폭**의 **균형적 설계**가 중요함.\n",
        "\n",
        "- **의의(Significance)**:  \n",
        "  RLHF는 SFT로 학습된 기본 언어모델을 기반으로, **RM(사람의 선호 점수)** 을 보상 신호로 사용해 **PPO 알고리즘**으로 정책을 점진적으로 개선하는 단계임.  \n",
        "  이 과정을 통해 모델은 **정확성, 유용성, 예의, 안전성**을 모두 고려한 인간 친화적 응답을 생성하게 됨.  \n",
        "\n",
        "  대표적인 예로 **ChatGPT(3.5)**, **Gemma-IT**, **LLaMA-2-Chat** 등이 있으며,  \n",
        "  이들 모델은 \"요약해줘\", \"코드 짜줘\", \"쉽게 설명해줘\" 등 사람의 지시를 자연스럽게 이해하고 수행함과 동시에,  \n",
        "  **윤리적이고 안전한 대화**를 유지하도록 설계되어 있음.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d25814f",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 24
        }
      },
      "source": [
        "### 2.3 주요 이슈\n",
        "\n",
        "#### 1) 기술적 이슈: 할루시네이션(Hallucination)\n",
        "<div style=\"display: flex; align-items: flex-start;\">\n",
        "  <img src=\"image/Hallucination.png\" width=\"400\" style=\"margin-right: 20px;\"/>\n",
        "\n",
        "  <div>\n",
        "\n",
        "모델이 **사실이 아닌 정보를 그럴듯하게 만들어내는 현상**  \n",
        "예: 존재하지 않는 논문을 인용하거나, 틀린 수학 답을 자신 있게 제시  \n",
        "원인: 모델이 \"언어적 패턴\"만 학습하고, **사실 검증 능력**은 갖추지 못했기 때문  \n",
        "\n",
        "**최근 개선 방향**\n",
        "- 최신 모델들은 **내부적으로 할루시네이션 억제 장치**(안전망, 출력 검증 모듈 등)를 포함함  \n",
        "- **외부 검색 결합**을 통해 실제 정보와 비교하도록 보완함  \n",
        "  - **웹 검색 결합형 모델**  \n",
        "  - **RAG (Retrieval-Augmented Generation)** : 검색 결과를 모델 입력에 함께 제공하여 정확도 향상\n",
        "  </div>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "7686ae49",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 25
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "모델 응답: 유재석의 형의 이름은 유황제이입니다.\n",
            "모델 응답: 2015년 노벨문학상을 수상한 대한민국 사람은 한강이야.\n"
          ]
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI\n",
        "import os\n",
        "\n",
        "# .env 로드 (.env 파일에 OPENAI_API_KEY가 있어야 합니다)\n",
        "load_dotenv()\n",
        "#api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",  # 사용 모델\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"대한민국 코미디언 유재석의 형 이름은 뭐야?\"}\n",
        "    ],\n",
        "    max_tokens=500,\n",
        ")\n",
        "\n",
        "print(\"모델 응답:\", response.choices[0].message.content)\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",  # 사용 모델\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"2015년 노벨문학상을 수상한 대한민국 사람은 누구야?\"}\n",
        "    ],\n",
        "    max_tokens=500,\n",
        ")\n",
        "\n",
        "print(\"모델 응답:\", response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f5e96fa",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 26
        }
      },
      "source": [
        "참고 : 할루시네이션을 줄이기 위한 기타 방지 전략\n",
        "\n",
        "| 전략 | 설명 |\n",
        "|------|------|\n",
        "| 프롬프트 구체화 | \"사실 기반으로만 답변해줘\n",
        "검증 가능한 정보만 알려줘\"처럼 요청을 명확히 작성해 모델의 임의 추론을 줄임. |\n",
        "| 지식 시점 명확히 지정 | \"2023년 기준으로 설명해줘\"처럼 기준 연도를 알려 오래된 정보로 인한 오류를 줄임. |\n",
        "| 역할(Role) 지시 강화 | system 메시지에 \"당신은 신중하고 사실에 기반해 답변하는 어시스턴트입니다\"처럼 모델의 태도를 명확히 지시. |\n",
        "| RAG 방식 적용 | Retrieval-Augmented Generation을 활용해 실제 문서나 DB를 기반으로 답변하도록 유도. |\n",
        "| 최종 검증 절차 포함 | 모델이 생성한 내용은 반드시 사람이 추가 확인·검증한 뒤 활용하는 것을 원칙으로 함. |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "e0da0103",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 27
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "모델 응답: 유재석은 개인적인 사생활을 철저히 보호하고 있기 때문에 그의 가족에 대한 정보는 공식적으로 크게 알려져 있지 않습니다. 그러나 유재석은 자녀들과 부인에 대한 언급을 통해 이들이 있다는 사실을 알 수 있습니다. 관련된 뉴스나 소문들을 통해 일부 정보를 알 수도 있지만, 그 가족에 대한 자세한 정보는 사생활을 존중하는 관행에 따라 공개되지 않는 것이 일반적입니다.\n",
            "모델 응답: 2020년 노벨문학상은 루이스 글루크만이 수상했습니다. 대한민국 출신 인물은 없습니다.\n"
          ]
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI\n",
        "import os\n",
        "\n",
        "# .env 로드 (.env 파일에 OPENAI_API_KEY가 있어야 합니다)\n",
        "load_dotenv()\n",
        "#api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",  # 사용 모델\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"대한민국 코미디언 유재석의 가족에 대한 정보는 어디까지 공식적으로 알려져 있어?\"}\n",
        "    ],\n",
        "    max_tokens=500,\n",
        ")\n",
        "\n",
        "print(\"모델 응답:\", response.choices[0].message.content)\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",  # 사용 모델\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"2020년 노벨문학상 수상자 중 대한민국 출신 인물이 있는지 확인해서 알려줘. 없다면 없다고 답변해줘.\"}\n",
        "    ],\n",
        "    max_tokens=500,\n",
        ")\n",
        "\n",
        "print(\"모델 응답:\", response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92448e24",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 28
        }
      },
      "source": [
        "#### 2) 윤리적 이슈: 개인정보·저작권·투명성\n",
        "<img src=\"image/이루다.jpg\" width=\"400\">\n",
        "\n",
        "- **개인정보**: 학습 데이터에 민감한 개인 정보(이메일, 전화번호 등)가 포함될 수 있음  \n",
        "- **저작권**: 책, 논문, 코드 등의 무단 학습이 저작권 침해 논란으로 이어짐  \n",
        "- **투명성**: 데이터 출처나 모델 구조가 공개되지 않으면 신뢰성 저하  \n",
        "→ 이에 따라 기업과 연구기관은 **데이터 필터링, 안전한 학습 데이터 구축, 투명한 공개 정책**을 강화하는 추세"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe9eff70",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 29
        }
      },
      "source": [
        "#### 3) 편향(Bias) 및 부적절한 응답 문제\n",
        "모델은 학습 데이터의 특성과 구조적 한계로 인해  \n",
        "**특정 집단·성별·인종에 대한 편향된 표현**을 사용하거나,  \n",
        "**유해·공격적·부적절한 발화**를 생성하는 문제가 발생할 수 있음.\n",
        "\n",
        "이러한 편향 문제는 실제 서비스 환경에서  \n",
        "신뢰성 저하, 사회적 논란, 윤리적 위험으로 이어질 수 있어  \n",
        "가장 중요한 AI 안전 이슈 중 하나로 다뤄짐.\n",
        "\n",
        "**대표적인 대응 기술 (Model Alignment)**\n",
        "- **FLAN**: 다양한 지시문 기반 학습으로 모델의 응답 방향성을 인간 선호에 맞추는 방식  \n",
        "- **InstructGPT / RLHF**: 사람이 평가한 피드백을 강화학습에 적용하여  \n",
        "  모델이 **더 안전하고 자연스러운 답변**을 생성하도록 조정  \n",
        "- **Constitutional AI**: ‘규칙 기반 원칙’을 설정하고,  \n",
        "  이를 기준으로 모델 스스로 출력을 조정하도록 하는 방식\n",
        "\n",
        "**목표**\n",
        "- 사용자의 질문 의도를 정확히 반영하며  \n",
        "  **편향을 최소화하고, 안전하고 윤리적인 응답을 생성하는 것** "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1419e120",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 30
        }
      },
      "source": [
        "### 정리\n",
        "- LLM은 텍스트를 이해하고 생성하는 **초거대 인공지능 모델**  \n",
        "- Transformer 구조와 Scaling Law 덕분에 빠른 성능 발전을 이루었음  \n",
        "- Instruct 모델의 등장으로, 단순 언어 모델을 넘어 **사용자 지시를 따르는 대화형 모델**이 가능해짐  \n",
        "- 주요 이슈(할루시네이션, 개인정보, 저작권)를 이해하고, 보완책(RAG, Alignment 등)을 아는 것이 중요  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ec6c18c",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 31
        }
      },
      "source": [
        "## 3. 최신 LLM 기술 동향"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53db1102",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 32
        }
      },
      "source": [
        "### 3.1 빅테크 기업들의 LLM 전략 (2025년 기준)\n",
        "\n",
        "2025년 현재, 주요 빅테크 기업들은 LLM을 중심으로 **통합 에이전트화, 멀티모달 확장, 오픈소스 혁신, 강화된 안전성**이라는  \n",
        "네 가지 방향에서 뚜렷한 전략적 차별화를 보이고 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f8498b8",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 33
        }
      },
      "source": [
        "####  OpenAI: 혁신적 통합형 에이전트와 도구 중심 발전\n",
        "OpenAI는 GPT-4 이후 **o3, o4-mini, GPT-5 시리즈** 등을 연달아 공개하며  \n",
        "\"대화형 모델\"을 넘어선 **통합형 에이전트(Agentic AI)** 전략으로 전환하고 있습니다.  \n",
        "\n",
        "- **통합형 도구 활용**:  \n",
        "  ChatGPT에 내장된 웹검색, 파일 / 데이터 분석(Python), 이미지 / 비디오 생성(Sora) 등  \n",
        "  다양한 기능을 복합적으로 연결하여 실제 업무 · 교육 · 연구에서 **문제 해결형 에이전트**로 진화.  \n",
        "- **모델 다양화**:  \n",
        "  GPT-5, GPT-5-mini, GPT-5-nano 등은 API 형태로 제공되어  \n",
        "  사용 목적에 따라 **추론력과 경량화 수준을 선택적으로 조정** 가능.  \n",
        "- **산업 확장**:  \n",
        "  RAG(검색결과 기반 생성), 자동화 백오피스, 대규모 콘텐츠 / 데이터 파이프라인 구축 등  \n",
        "  **엔터프라이즈 중심의 실무형 LLM 솔루션**을 확장 중.  \n",
        "- **특화형 모델**:  \n",
        "  Sora(비디오 생성), Codex Agent(코딩 자동화) 등 멀티모달 · 도구 에이전트 분야 집중."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53e0d1e6",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 34
        }
      },
      "source": [
        "#### Google: Gemini 기반 멀티모달 · 검색 생태계 확장\n",
        "Google은 **Gemini 2.5 Pro, Flash, Flash-Lite** 등  \n",
        "최신 Gemini 시리즈를 중심으로 **\"모든 미디어를 이해하는 AI 플랫폼\"** 구축에 주력하고 있습니다.  \n",
        "\n",
        "- **멀티모달 통합**:  \n",
        "  텍스트, 이미지, 음성, 비디오 입력을 한 대화 흐름에서 자연스럽게 연결하며  \n",
        "  코드 생성 · 영상 분석 · 오디오 이해 등 복합 작업을 수행.  \n",
        "- **검색 혁신**:  \n",
        "  Gemini 기반의 **AI Search Labs 모드**를 통해  \n",
        "  텍스트 + 이미지 + 음성 검색이 결합된 차세대 검색 경험을 15억 명 이상에게 제공.  \n",
        "- **산업 적용**:  \n",
        "  교육, 금융, 엔터테인먼트 등 B2B / B2C 분야에서  \n",
        "  개인화 학습, 문서 요약, 상담 분석 등에 적극 활용.  \n",
        "- **효율성 강화**:  \n",
        "  Flash 모델군은 속도·비용 · 문맥 처리 효율을 동시에 개선하여  \n",
        "  **경량형 LLM 성능 최적화 전략**을 실현."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ba28411",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 35
        }
      },
      "source": [
        "#### Meta: 오픈소스로 혁신 확산 · 차세대 대용량 모델 공개\n",
        "Meta는 **LLaMA 3.1 (405 B)** 및 **LLaMA 4(Scout, Maverick)** 등  \n",
        "초대형 오픈소스 LLM을 무료 라이선스로 공개하며 **AI 생태계 개방 전략**을 강력히 추진하고 있습니다.  \n",
        "\n",
        "- **대용량 모델 공개**:  \n",
        "  LLaMA 3.1-405B는 4,050억 파라미터로 현재 공개 모델 중 최대 규모이며,  \n",
        "  GPT-4 Turbo나 Claude 3 수준의 성능을 보임.  \n",
        "- **개발자 친화성**:  \n",
        "  Hugging Face 등 플랫폼에서 모델을 자유롭게 다운로드 · 배포 가능하며  \n",
        "  연구자·교육자·기업이 직접 활용할 수 있도록 설계.  \n",
        "- **전략적 목표**:  \n",
        "  모델을 무료로 풀어 **플랫폼 통합 · R&D 비용 절감 · 커뮤니티 확장**을 촉진.  \n",
        "- **오픈소스 효과**:  \n",
        "  합성 데이터 생성, 외부 모델 검증, AI 평가 표준화 등에서  \n",
        "  **산업 전반의 혁신 가속화**에 기여.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5e236a7",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 36
        }
      },
      "source": [
        "#### Anthropic: Claude Sonnet 4.5의 강화된 안전성 · 정밀 Alignment\n",
        "Anthropic은 **Claude 시리즈(최근 Sonnet 4.5)** 를 통해  \n",
        "AI의 **안전성(Alignment)** 과 **책임 있는 확장(Responsible Scaling)** 을 핵심 가치로 삼고 있습니다.  \n",
        "\n",
        "- **안전성 강화**:  \n",
        "  Claude Sonnet 4.5는 AI 위험 행동(속임수, 프롬프트 공격 등)을 탐지·차단하는  \n",
        "  자동 진단 · 필터링 로직을 내장, Anthropic의 **Level 3 Safety 등급** 획득.  \n",
        "- **행동 감사 시스템**:  \n",
        "  내부 ‘Behavioral Audit’ 체계를 통해 모델 행동을 유형별로 모니터링 하고,  \n",
        "  안전 위반 시 자동 수정 · 피드백 루프 를 적용.  \n",
        "- **기업 중심 전략**:  \n",
        "  법률 · 금융 · 행정 환경에서 요구되는 규제 준수 및 통제 구조를 내장하여  \n",
        "  **책임 기반 AI 운영 플랫폼**으로 활용 범위 확대.  \n",
        "- **기술적 특징**:  \n",
        "  코드 생성 · 자동화 에이전트 · 고위험 도메인 컨텐츠 필터링 분야에서  \n",
        "  높은 신뢰성과 세밀한 대화 제어 능력을 보임."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1ad489e",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 37
        }
      },
      "source": [
        "#### 빅테크 LLM 전략 비교 (2025년 기준)\n",
        "\n",
        "| 기업 | 최신 모델 / 제품 | 전략 키워드 | 주요 특징 |\n",
        "|------|------------------|--------------|------------|\n",
        "| **OpenAI** | o3, o4-mini, GPT-5, Sora | 에이전트 · 통합도구 | 웹검색, 파일분석, 멀티모달 / 비디오 생성, API·엔터프라이즈·RAG 확장 |\n",
        "| **Google** | Gemini (2.5 Pro, Flash 등) | 멀티모달 · 검색 생태계 | 텍스트 · 이미지 · 음성 · 비디오 통합, 속도·문맥처리 개선, 검색 및 산업 적용 |\n",
        "| **Meta** | LLaMA 3.1-405B, LLaMA 4 | 오픈소스 생태계 확산 | 대용량 무료 모델, 다운로드 · 배포 자유, 합성데이터 · 외부 검증 확장 |\n",
        "| **Anthropic** | Claude Sonnet 4.5 | 안전성 · 정밀 Alignment | Level 3 Safety, 오용 자동감지, 법률·기업 규제 적합성, 에이전트 특화 대화 및 코드 생성 |\n",
        "\n",
        "\n",
        "이처럼 2025년 현재 각 빅테크 기업은  \n",
        "**통합 AI 에이전트 (OpenAI)**, **멀티모달 검색 플랫폼 (Google)**, **오픈소스 혁신 확산 (Meta)**,  \n",
        "**강화된 안전성 및 책임성 (Anthropic)** 을 핵심 축으로 삼아 AI 생태계의 주도권을 경쟁하고 있습니다.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "914f9eb3",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 38
        }
      },
      "source": [
        "### 3.2 오픈소스 LLM\n",
        "<img src=\"image/llama.png\" width=\"450\">\n",
        "\n",
        "이미지 출처 : https://discuss.pytorch.kr/t/meta-llama-2-7b-13b-70b-4k-context/2095\n",
        "- **LLaMA (Meta)**: 연구용으로 공개, 많은 변형 모델(Alpaca, Vicuna 등) 탄생  \n",
        "- **Mistral**: 효율성과 성능을 동시에 잡은 경량 LLM  \n",
        "- **Gemma (Google)**: 연구와 교육을 위한 소형 LLM, 누구나 실험 가능  \n",
        "\n",
        "오픈소스 LLM은 **모델 구조와 가중치가 공개되어 있어 투명성과 확장성**이 높습니다.  \n",
        "다만 직접 모델을 운영하려면 **고성능 GPU와 전력 비용**이 필요하므로 완전히 저비용이라 하긴 어렵습니다.  \n",
        "\n",
        "그럼에도 불구하고 오픈소스 LLM은 **데이터 보안과 맞춤형 기능 구현** 측면에서 높은 활용 가치를 가집니다.  \n",
        "**RAG(Retrieval-Augmented Generation)**, **프롬프트 엔지니어링**, **파인튜닝(fine-tuning)** 등을 통해  \n",
        "특정 도메인(예: 법률, 의료, 제조 등)에 특화된 모델로 최적화할 수 있습니다.  \n",
        "\n",
        "특히 **삼성, SK 등 대기업**은 상용 LLM을 사용할 경우 **자사 기밀 데이터가 외부로 유출될 위험**이 있어,  \n",
        "오픈소스 LLM을 기반으로 자체 인프라에 설치하고 **사내 전용 챗봇**으로 활용 중입니다.  \n",
        "이 과정에서 내부 문서 검색과 RAG 기술을 결합하거나, 업무별 프롬프트를 설계해 **보안성과 효율성**을 동시에 확보하고 있습니다.  \n",
        "\n",
        "결국, 단순히 LLM을 ‘사용’하는 것을 넘어 **오픈소스 모델을 직접 다루고 최적화할 수 있는 능력**이  \n",
        "기업과 연구기관 모두에서 **핵심 기술 역량**으로 평가받고 있습니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0763c409",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 39
        }
      },
      "source": [
        "#### Llama 4 시리즈(Meta)\n",
        "\n",
        "Meta의 최신 오픈소스 LLM으로, **단일 모델이 아닌 여러 하위 버전(라인업)** 으로 구성되어 있습니다.  \n",
        "Llama 4는 **Mixture-of-Experts(MoE) 구조**, **초장문 컨텍스트 처리 능력**, **멀티모달 입력 지원** 등  \n",
        "최신 대형 모델 기술이 종합적으로 적용된 시리즈 입니다.\n",
        "\n",
        "대표적으로  \n",
        "  - **Llama 4 Scout** : 초장문(최대 10M 토큰) 지원, 장거리 문맥 처리에 특화  \n",
        "  - **Llama 4 Maverick** : 1M 토큰급 중간 버전, 범용·고효율형  \n",
        "  - **Llama 4 Guardian** : 안정성과 정확도 중심의 대형 모델  \n",
        "\n",
        "각 모델은 사용 목적(연구·제품·경량 배포 등)에 따라 **성능·용량·속도**가 다르게 설계되어 있습니다. \n",
        "\n",
        "\n",
        "**Mixture-of-Experts(MoE) 구조**  \n",
        "<img src=\"image/moe.png\" width=\"500\">  \n",
        "\n",
        "https://huggingface.co/blog/moe\n",
        "  - 겉으로는 하나의 큰 모델처럼 보이지만, 안쪽에는 여러 ‘전문가(Expert) 네트워크’가 들어 있는 구조.  \n",
        "  - 겉보기에는 하나의 거대한 모델처럼 동작하지만, 내부에는 여러 개의 **전문가(Expert) 네트워크**가 존재한다.  \n",
        "  - 입력이 들어올 때마다, 모델은 가장 적합한 전문가만 선택해 계산을 수행하기 때문에  \n",
        "    **전체 파라미터 중 일부만 활성화되어 효율적으로 연산**이 이루어진다.  \n",
        "  - 이 덕분에 **성능은 유지하면서도 학습 및 추론 비용을 크게 줄일 수 있는 구조**를 갖는다.\n",
        "\n",
        "**초장문 컨텍스트 처리**  \n",
        "  - Llama 4 **Scout**: 최대 약 **1,000만 토큰(10M)** 수준의 컨텍스트 지원  \n",
        "  - Llama 4 **Maverick**: 약 **100만 토큰(1M)** 컨텍스트 지원  \n",
        "  - *컨텍스트 길이*란, 한 번에 모델이 \"읽고 기억하면서\" 처리할 수 있는 글자 수 같은 개념입니다.  \n",
        "    → 예를 들어, **수천~수만 페이지 분량의 문서나 대규모 코드 저장소 전체를 한 번에 넣고 분석**하는 식의 활용이 가능하다.\n",
        "\n",
        "**멀티모달 & 고성능 범용 모델**  \n",
        "  - 텍스트뿐 아니라 **이미지까지 같이 입력**받을 수 있는 멀티모달 모델로 설계됨.  \n",
        "  - 챗봇, 코드 분석, 문서 요약, 이미지 설명 생성 등 **범용 고성능 모델**로 사용 가능하다.\n",
        "  - 이러한 초장문 처리는 RAG 시스템, 대규모 문서 검색, 장기 문맥 유지형 챗봇 등에 매우 유용하다.\n",
        "\n",
        "**멀티모달 확장성**  \n",
        "  - Llama 4는 텍스트뿐 아니라 **이미지 입력까지 함께 처리**할 수 있도록 설계된 멀티모달 모델입니다.  \n",
        "  - 따라서 단순한 대화형 모델을 넘어, **이미지 설명 생성·문서 시각자료 해석·코드 분석·지식 응용** 등  \n",
        "    다양한 AI 응용 분야에 활용할 수 있습니다.\n",
        "\n",
        "요약하자면, **Llama 4는 MoE 기반의 효율성, 초장문 컨텍스트 처리, 멀티모달 확장성**을 모두 갖춘  \n",
        "**범용형 오픈소스 LLM 시리즈**로, 연구와 산업 현장에서 동시에 높은 주목을 받고 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a825418",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 40
        }
      },
      "source": [
        "#### Qwen3 (Alibaba)\n",
        "\n",
        "<img src=\"image/qwen3.jpeg\" width=\"550\">\n",
        "\n",
        "이미지 출처 : https://discuss.pytorch.kr/t/qwen3/6880\n",
        "\n",
        "Alibaba가 개발한 **차세대 오픈소스 LLM 시리즈**로, 이전 세대(Qwen2·Qwen2.5) 대비 **추론력, 코딩 능력, 긴 컨텍스트 처리 성능**이 크게 향상된 것이 특징입니다.  \n",
        "Qwen3는 **Dense(일반형)** 과 **Mixture-of-Experts(MoE) 구조 모델**을 모두 포함하는 폭넓은 라인업으로 구성되어 있으며,  \n",
        "**대형 상용 모델에 근접한 성능을 오픈소스 형태로 제공한다**는 점에서 큰 주목을 받고 있습니다.\n",
        "\n",
        "\n",
        "**다양한 크기와 구조 선택 (Dense + MoE 동시 제공)**  \n",
        "  - Qwen3는 수억 개 파라미터의 **경량 모델(예: Qwen3-1.8B)** 부터,  \n",
        "    수백억·수천억 파라미터를 가진 **대형 모델(Qwen3-72B, Qwen3-MoE 등)** 까지 다양한 버전을 제공합니다.\n",
        "\n",
        "  - 특히 **MoE 버전**은 내부적으로 여러 전문가 네트워크를 두고 입력에 따라 일부만 활성화시키는 구조로,  \n",
        "    **속도와 성능을 모두 잡은 효율형 설계**를 지향합니다. \n",
        "\n",
        "  - 사용자는 목적에 따라 **\"가볍지만 빠른 모델\"** 혹은 **\"무겁지만 정밀한 모델\"** 중 선택할 수 있습니다.  \n",
        "    → 예: 노트북 실험용이라면 7B 이하 모델, 서버 운영용이라면 32B~72B 모델을 주로 활용합니다.\n",
        "\n",
        "\n",
        "\n",
        "**긴 컨텍스트 + 강화된 추론·코딩 능력**  \n",
        "  - Qwen3는 최대 **10만 토큰(약 10만 단어 수준)** 을 처리할 수 있는 **Long-Context 모델**을 지원합니다. \n",
        "\n",
        "  - 이 덕분에 **대규모 문서, 긴 대화 이력, 코드 저장소 전체**를 한 번에 분석하거나, **문맥 유지형 QA·문서 요약·RAG 기반 질의응답** 등에 활용하기 유리합니다. \n",
        "\n",
        "  - 수학, 논리, 코딩 벤치마크(MATH, GSM8K, HumanEval 등)에서 이전 세대(Qwen2.5)보다 **한층 향상된 성능을 기록**했으며,  \n",
        "    특히 **코드 생성·디버깅·테스트 케이스 생성** 등 개발자 중심 작업에 최적화되어 있습니다.\n",
        "\n",
        "\n",
        "\n",
        "**개방형 생태계와 풍부한 개발 도구 지원**  \n",
        "  - Hugging Face, ModelScope, Colab 등 주요 플랫폼에 **Qwen3 전용 체크포인트, 데모, 튜토리얼 코드**가 제공됩니다.  \n",
        "\n",
        "  - 초심자도 공식 가이드를 따라하면 **간단한 챗봇·코딩 어시스턴트·문서 요약기** 등을 직접 구축할 수 있습니다. \n",
        "\n",
        "  - 또한 Python SDK, CLI 도구, API 엔드포인트 등도 함께 제공되어,  \n",
        "    **로컬 환경에서의 실험부터 서버형 애플리케이션 통합까지 쉽게 구현이 가능합니다.** \n",
        "\n",
        "  - 오픈 커뮤니티 중심으로 빠르게 발전하고 있으며,  \n",
        "    모델 업데이트나 성능 개선이 활발하고 **실제 서비스 적용 사례**도 꾸준히 늘어나고 있습니다.\n",
        "\n",
        "\n",
        "**Qwen3는 긴 문맥 이해력, 강력한 코딩·추론 성능, 유연한 모델 구조**를 모두 갖춘 **실용 중심 오픈소스 LLM 시리즈**입니다.  \n",
        "특히 \"자체 서비스나 로컬 환경에서 직접 모델을 운용하고자 하는 개발자·연구자\"에게 가장 균형 잡힌 선택지 중 하나로 평가받고 있습니다.\n",
        "\n",
        "Qwen3 깃허브 저장소 : https://github.com/QwenLM/Qwen3?utm_source=pytorchkr&ref=pytorchkr"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26f8ce1f",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 41
        }
      },
      "source": [
        "#### 참고"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4378995",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 42
        }
      },
      "source": [
        "```python\n",
        "from modelscope import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name = \"Qwen/Qwen3-30B-A3B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"auto\", device_map=\"auto\")\n",
        "\n",
        "prompt = \"대형 언어 모델에 대해 간단히 소개해 주세요.\"\n",
        "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True, enable_thinking=True)\n",
        "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "generated_ids = model.generate(**model_inputs, max_new_tokens=32768)\n",
        "output = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "print(output)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3c0ed77",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 43
        }
      },
      "source": [
        "#### Gemma 시리즈\n",
        "\n",
        "Google이 공개한 **경량 오픈소스 LLM 시리즈**로, 대형 모델 **Gemini**의 기술적 기반을 이어받아 **효율성과 접근성**을 동시에 확보한 것이 특징입니다.  \n",
        "Gemma는 **소형·중형 규모(수십억 파라미터급)** 모델 중심으로 설계되어,  \n",
        "**연구, 교육, 로컬 실험 환경에서 직접 구동 가능한 LLM**을 목표로 하고 있습니다.\n",
        "\n",
        "\n",
        "\n",
        "**Gemini 기술 기반의 경량 오픈 모델 패밀리**  \n",
        "  - Gemma는 Google의 초대형 모델 **Gemini**와 동일한 연구 프레임워크와 학습 방법론을 바탕으로 개발되었습니다.  \n",
        "  - 다만 **규모를 줄이고, 오픈 접근성을 강화**하여 누구나 실험·연구·프로토타입 개발에 활용할 수 있도록 설계되었습니다.  \n",
        "  - 주력 모델은 **Gemma 2B, 7B, 9B** 등으로,  \n",
        "    **너무 무겁지 않으면서도 충분한 품질을 제공하는 실용형 모델**이라는 평가를 받고 있습니다.  \n",
        "  - 최신 버전인 **Gemma 2**와 **Gemma 3 Nano**는  \n",
        "    파라미터 수 대비 **추론 효율(throughput)과 메모리 사용률**이 크게 개선되어 있습니다.\n",
        "\n",
        "\n",
        "\n",
        "**온디바이스 및 로컬 실행 최적화**  \n",
        "  - Gemma 시리즈는 **온디바이스(on-device) LLM 구동**을 고려하여 설계된 것이 가장 큰 특징입니다.  \n",
        "  - 즉, **노트북·워크스테이션·일부 고성능 모바일 기기**에서도 실행이 가능하며,  \n",
        "    완전히 클라우드에 의존하지 않고 **로컬 환경에서 LLM을 직접 운용**할 수 있습니다.  \n",
        "  - 이러한 특성은 **데이터 보안과 개인정보 보호**가 중요한 기업·기관 환경에서 특히 유용합니다.  \n",
        "  - 예를 들어, 사내 네트워크 안에서 문서 요약, 내부 보고서 QA, RAG 검색형 챗봇 등을 구현할 수 있습니다.\n",
        "\n",
        "\n",
        "\n",
        "**개발자 친화적 오픈웨이트 구조**  \n",
        "  - Gemma는 모델 가중치(Weights)와 함께 **예제 코드, 튜닝 가이드, RAG·프롬프트 엔지니어링 템플릿**을 함께 제공합니다.  \n",
        "  - 이를 통해 개발자는 특정 도메인(법률, 의료, 제조 등)에 맞는 **전용 Gemma 챗봇이나 분석기**를 쉽게 제작할 수 있습니다.  \n",
        "  - 또한 PyTorch, JAX, TensorFlow 등 주요 프레임워크를 모두 지원하여 **개발 환경 호환성**이 뛰어납니다.  \n",
        "  - 성능 면에서는 초대형 모델보다는 가볍지만,  \n",
        "    **연구·교육·사내 파일 요약·프로토타입 서비스 개발** 등 실제 현장에서 활용하기에 매우 적합합니다.\n",
        "    \n",
        "  \n",
        "요약하자면, **Gemma는 Gemini의 기술을 계승하면서도 경량화와 개방성을 강화한 실용형 오픈소스 LLM 시리즈**입니다.  \n",
        "**로컬에서 직접 돌릴 수 있는 효율적 모델**, **프롬프트 및 RAG 실험 플랫폼**, **교육·연구용 기본 모델**로서  \n",
        "다양한 환경에서 활용도가 매우 높습니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "308625ef",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 44
        }
      },
      "source": [
        "#### 정리\n",
        "\n",
        "- **Llama 4**  \n",
        "  - 키워드: **초장문(10M 토큰), 멀티모달, MoE, 프론티어급 성능**  \n",
        "  - \"수많은 문서나 코드 전체를 한 번에 처리할 수 있는\" 고성능 범용 모델입니다.\n",
        "\n",
        "- **Qwen3**  \n",
        "  - 키워드: **코딩·추론 강점, 긴 컨텍스트, 다양한 크기·MoE**  \n",
        "  - \"코드, 수학, 논리 작업을 수행하는 개발자·연구자\"에게 적합한 실용적 오픈소스 시리즈입니다.\n",
        "\n",
        "- **Gemma 최신 모델**  \n",
        "  - 키워드: **경량, 온디바이스(PC·모바일), Gemini 기술 기반**  \n",
        "  - \"노트북이나 로컬 환경에서 직접 실행하며 실험하기 좋은\" 경량·효율형 모델입니다. \n",
        "\n",
        "이처럼 주요 오픈소스 LLM의 특성을 이해해두면, 향후 **로컬 LLM을 구축하거나 사내 전용 모델을 개발할 때 어떤 모델을 기반(Base)으로 선택할지** 판단하는 데 큰 도움이 됩니다.  \n",
        "\n",
        "즉, 각 모델의 강점을 파악해 목적에 맞는 **LLM 아키텍처를 설계**하고, 필요에 따라 **RAG나 프롬프트 엔지니어링을 결합해 효율적으로 확장**할 수 있습니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0fac5f05",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 45
        }
      },
      "source": [
        "### 3.3 LLM 활용 산업 트렌드\n",
        "\n",
        "- **챗봇**: 고객 응대, 개인 비서, 학습 튜터 등 대화형 서비스에 활용됩니다.  \n",
        "- **코드 생성**: GitHub Copilot, ChatGPT Code Interpreter 등 개발자 생산성을 높이는 도구로 발전했습니다.  \n",
        "- **멀티모달 AI**: 텍스트·이미지·음성을 동시에 이해하고 처리하는 기술로, Gemini와 GPT-4가 대표적입니다.  \n",
        "- **AI 에이전트**: 일정 관리, 문서 정리, 웹 검색, 데이터 분석 등 **자동화된 업무 수행**이 가능해졌습니다.  \n",
        "- **전문 분야**: 헬스케어 상담, 법률 문서 분석, 금융 데이터 리포트 생성 등 **특화 산업 영역으로의 확장**이 활발합니다.  \n",
        "\n",
        "LLM은 이제 단순히 \"대화하는 AI\"에 머물지 않고, **산업 전반의 효율성과 생산성을 혁신하는 핵심 기술**로 자리 잡고 있습니다.  \n",
        "기업들은 이를 통해 반복 업무를 자동화하고, 맞춤형 서비스를 제공하며, 기존 시스템과의 통합을 추진하고 있습니다.\n",
        "\n",
        "\n",
        "### 정리\n",
        "\n",
        "- 언어 모델은 **n-gram → RNN → Transformer → LLM** 순으로 발전했습니다.  \n",
        "- **GPT, BERT, PaLM, LLaMA** 등 다양한 계열의 모델이 현재 연구와 산업을 주도하고 있습니다.  \n",
        "- 최신 트렌드는 **빅테크의 초대형 모델과 오픈소스 LLM의 빠른 확산**으로 요약됩니다.  \n",
        "- 활용 범위는 챗봇을 넘어 **코드 생성, 멀티모달 처리, AI 에이전트, 전문 서비스 분야**까지 확장되었습니다.  \n",
        "\n",
        "이제부터는 다양한 **오픈소스 LLM과 상용 모델**을 바탕으로, 별도의 파인튜닝 없이 **프롬프트 설계만으로 모델을 효율적으로 커스텀하는 방법**,  \n",
        "즉 **프롬프트 엔지니어링(Prompt Engineering)의 원리와 전략**을 살펴보겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5067a16",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 46
        }
      },
      "source": [
        "## 4. 프롬프트 엔지니어링\n",
        "\n",
        "LLM을 제대로 활용하기 위해서는 단순히 \"질문을 잘 던지는 법\"을 아는 것만으로는 충분하지 않습니다.  \n",
        "**프롬프트 엔지니어링(Prompt Engineering)** 은 모델의 성능을 최대한 이끌어내기 위한 **설계 기술이자 전략적 커뮤니케이션 방법**입니다.  \n",
        "\n",
        "LLM은 단순한 대화형 모델이 아니라, **In-Context Learning(문맥 기반 학습)** 능력을 갖춘 시스템입니다.  \n",
        "즉, 모델은 입력된 긴 텍스트(맥락) 속에서 새로운 정보를 이해하고, 마치 학습된 것처럼 활용할 수 있습니다.  \n",
        "\n",
        "이 특성을 이용하면, **웹 검색 결과나 벡터 DB(Vector Database)에서 추출한 전문 지식**을 프롬프트에 함께 포함시켜  \n",
        "별도의 파인튜닝 없이도 **특정 분야에 특화된 커스텀 챗봇**을 구축할 수 있습니다.  \n",
        "\n",
        "따라서 프롬프트는 단순한 \"질문\"이 아니라, 모델의 사고 범위와 행동 방식을 설계하는 **핵심 인터페이스이자 지식 주입 통로**입니다.  \n",
        "프롬프트를 어떻게 구성하느냐에 따라, 같은 모델이라도 완전히 다른 성능을 보여줍니다.\n",
        "\n",
        "이 장에서는 프롬프트 엔지니어링의 개념과 주요 기법을 살펴보고, 직접 실습을 통해 **LLM을 커스터마이징하는 실질적인 방법**을 익혀봅니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bb84c13",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 47
        }
      },
      "source": [
        "### 4.1 프롬프트 엔지니어링의 개념과 중요성\n",
        "\n",
        "#### 왜 프롬프트가 중요한가?\n",
        "- LLM은 **확률적으로 다음 단어를 예측**하는 모델입니다.  \n",
        "- 같은 질문이라도 **맥락, 어휘 선택, 표현 방식**에 따라 전혀 다른 결과가 나올 수 있습니다.  \n",
        "- 원하는 답변을 얻으려면 **명확한 의도와 구조를 가진 프롬프트 설계**가 필요합니다.  \n",
        "- 나아가, 프롬프트는 단순한 입력이 아니라 **LLM의 사고 과정을 제어하는 명세서(Specification)** 역할을 합니다.\n",
        "\n",
        "즉, 프롬프트는 LLM과 상호작용하는 **가장 중요한 사용자 인터페이스(UI)** 입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4b3f4c5",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 48
        }
      },
      "source": [
        "### 4.2 프롬프트 설계 기법\n",
        "\n",
        "LLM을 효과적으로 활용하기 위해서는 단순한 질문이 아니라,  \n",
        "**명확한 역할과 지시, 그리고 일관된 출력 구조를 가진 프롬프트 설계**가 필요합니다.  \n",
        "이 절에서는 프롬프트의 기본 구성 요소와 원칙을 이해하고,  \n",
        "LangChain을 활용하여 실제로 프롬프트를 모델에 적용해보는 과정을 다룹니다. \n",
        "\n",
        "#### 프롬프트의 기본 구조\n",
        "\n",
        "프롬프트는 보통 다음 네 가지 요소로 구성됩니다.\n",
        "\n",
        "| 구성 요소 | 설명 | 예시 |\n",
        "|------------|------|------|\n",
        "| **역할(Role)** | 역할을 부여하여 모델의 관점을 설정 | \"너는 전문 번역가입니다.\" |\n",
        "| **맥락(Context)** | 상황·배경을 제공 | \"사용자는 공공기관 담당자입니다.\" |\n",
        "| **지시(Task)** | 수행할 작업을 명확히 지시 | \"다음 내용을 3줄로 요약하세요.\" |\n",
        "| **출력 형식(Output)** | 원하는 출력(결과) 형태를 지정 | \"표 형식으로 작성하세요.\" |\n",
        "\n",
        "이 구조는 LLM의 **In-Context Learning(문맥 기반 학습)** 특성과 직결됩니다.  \n",
        "명확한 구조를 제공하면 모델이 혼동 없이 목적에 맞는 출력을 생성합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "224a72a9",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 49
        }
      },
      "source": [
        "### 4.3 LangChain 소개\n",
        "\n",
        "### LangChain이란?\n",
        "\n",
        "<img src =\"image/LangChain.png\" width=\"500\">\n",
        "\n",
        "LangChain은 **대규모 언어모델(LLM)** 을 더 쉽게 활용할 수 있도록 도와주는 **프레임워크**입니다.  \n",
        "\n",
        "단순히 LLM에게 질문을 던지는 수준을 넘어서, **프롬프트 관리(prompt management)**,  \n",
        "**메모리(memory)**, **외부 도구(tool) 연동**, **문서 검색(RAG)** 등을 체계적으로 구성할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e20282f6",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 50
        }
      },
      "source": [
        "예를 들어, 우리가 ChatGPT 같은 LLM을 사용할 때  \n",
        "단순히 모델 하나만으로 뛰어난 답변이 만들어지는 것은 아닙니다.  \n",
        "\n",
        "실제로는 **검색(Search)**, **분석(Analysis)**, **정보 정리(Summarization)** 등  \n",
        "여러 절차가 백그라운드에서 함께 작동해야 더 정확하고 풍부한 답변을 얻을 수 있습니다.  \n",
        "\n",
        "LangChain은 이러한 여러 단계를 **하나의 체계적인 구조로 묶어주는 프레임워크**입니다.  \n",
        "즉, LLM이 단독으로 모든 일을 처리하는 대신  \n",
        "\"사용자 입력 → 관련 정보 검색 → 분석 및 요약 → 결과 생성\"과 같은  \n",
        "복잡한 흐름을 하나의 **체인(chain)** 으로 자동화할 수 있습니다.  \n",
        "\n",
        "<img src=\"image/chatgpt.jpg\" width=\"600\">\n",
        "\n",
        "예를 들어, 사용자가 \"3박 4일, 30만 원 예산, 맛집 중심의 서울 여행\"을 요청하면,  \n",
        "LangChain은 내부적으로 다음과 같은 과정을 수행합니다.  \n",
        "\n",
        "> 입력 분석 → 예산과 일정 파악 → 추천 장소 검색 → 일정표 생성 → 자연스러운 문장으로 요약  \n",
        "\n",
        "이처럼 LangChain은 단순한 대화 요청을 넘어 **검색, 분석, 조합, 요약** 등의 단계를 유기적으로 연결해  \n",
        "LLM이 더 똑똑하게 작동하도록 돕습니다.  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ef38cdb",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 51
        }
      },
      "source": [
        "또한 LangChain은 **LCEL (LangChain Expression Language)** 이라는 표현 언어를 통해  \n",
        "이러한 여러 단계(components)를 **직관적으로 연결**할 수 있게 해줍니다.  \n",
        "즉, \"LLM → 요약기 → 출력 포맷터\"처럼 **함수형 파이프라인**을 선언적으로 작성할 수 있어  \n",
        "확장성과 유지보수성이 뛰어납니다.\n",
        "\n",
        "LCEL 예시: \n",
        "```text\n",
        "사용자입력\n",
        "  | 분석기(요청에서 날짜·예산·키워드 추출)\n",
        "  | 검색기(분석 결과를 바탕으로 추천 장소 탐색)\n",
        "  | 일정생성기(추천 장소로 일자별 일정 구성)\n",
        "  | 요약기(자연스러운 문장으로 결과 정리)\n",
        "  | 출력\n",
        "```\n",
        "\n",
        "이처럼 LCEL은 각 단계를 함수처럼 \"연결(pipe)\"하여 데이터가 흘러가는 경로를 선언적으로 표현합니다.  \n",
        "복잡한 제어문 없이 \"입력 → 처리 → 출력\" 구조를 직관적으로 구성할 수 있어,\n",
        "체인 전체의 논리 흐름을 한눈에 파악할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f510c126",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 52
        }
      },
      "source": [
        "### 간단한 LangChain 예시 코드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "fa642ed0",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 53
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LangChain은 다양한 언어 모델을 연결하고 활용하여 자연어 처리 애플리케이션을 구축할 수 있도록 돕는 프레임워크입니다.\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import PromptTemplate # 문자열 템플릿 기반으로 프롬프트를 자동 구성해주는 유틸\n",
        "from langchain_core.output_parsers import StrOutputParser # LLM의 응답 객체를 문자열로 변환해주는 파서 (출력 후처리 단계)\n",
        "from dotenv import load_dotenv # .env 파일의 환경변수를 자동으로 불러오기 위한 모듈\n",
        "load_dotenv(override=True)  # 실행 시 .env 파일을 찾아 변수들을 환경에 로드\n",
        "\n",
        "# LCEL 방식: 파이프라인 형태로 선언\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "prompt = PromptTemplate.from_template(\"'{topic}' 주제에 대해 한 문장으로 설명해줘.\")\n",
        "output_str = StrOutputParser() # 결과 객체에서 텍스트(content)만 깔끔하게 추출해 문자열로 변환함\n",
        "\n",
        "# LCEL 표현 (프롬프트 → LLM → 출력)\n",
        "\n",
        "chain = prompt | llm | output_str\n",
        "\n",
        "# chain.invoke() 는 전체 파이프라인을 한 번 실행하는 메서드\n",
        "result = chain.invoke({\"topic\": \"LangChain\"})\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d450d87",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 54
        }
      },
      "source": [
        "### 실습 1: 기본 프롬프트 vs. 구조화 프롬프트 비교\n",
        "\n",
        "아래 예시는 같은 요청을 단순 프롬프트와 구조화된 프롬프트로 비교하는 예제입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "107dbc94",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 55
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===== [단순 프롬프트 결과] =====\n",
            "디지털 트윈(Digital Twin)은 물리적 객체, 시스템, 프로세스의 가상 시뮬레이션이나 복제 모델을 의미합니다. 즉, 실제 세계의 사물을 디지털 환경에서 실시간으로 반영하는 모델입니다. 디지털 트윈은 센서 데이터를 기반으로 하여 물리적 객체의 상태를 모니터링하고 분석하며, 예측 및 최적화하는 데 도움을 줄 수 있습니다.\n",
            "\n",
            "디지털 트윈의 주요 용도는 다음과 같습니다:\n",
            "\n",
            "1. **모니터링**: 실제 시스템의 상태와 성능을 실시간으로 추적하고 분석합니다.\n",
            "2. **예측 분석**: 데이터 분석을 통해 미래의 동향이나 문제를 예측할 수 있습니다.\n",
            "3. **최적화**: 운영 효율성을 높이기 위해 시스템이나 프로세스를 개선하는데 도움을 줍니다.\n",
            "4. **시뮬레이션**: 다양한 시나리오를 테스트하여 대안을 모색하고 의사결정을 지원합니다.\n",
            "\n",
            "예를 들어, 제조업에서는 생산 라인의 디지털 트윈을 만들어 실시간 데이터와 시뮬레이션을 통해 효율성을 증가시키고 유지보수 시점을 예측할 수 있습니다. 스마트 시티, 자동차, 헬스케어 등 다양한 산업에서 활용되고 있습니다. \n",
            "\n",
            "===== [구조화 프롬프트 결과] =====\n",
            "디지털 트윈은 현실에 있는 물건이나 장소를 컴퓨터 속에서 똑같이 만들어낸 모델이에요. 이렇게 만들어진 디지털 쌍둥이는 실제 상황을 잘 이해하고 문제를 해결하는 데 도움을 줘요. 예를 들어, 도시의 교통을 관리하는데 디지털 트윈을 사용하면, 차들이 어떻게 움직이는지 미리 보면서 더 좋은 길을 만들 수 있어요.\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.messages import HumanMessage\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# 모델 로드\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "\n",
        "# 단순 프롬프트\n",
        "basic_prompt = ChatPromptTemplate.from_template(\"디지털 트윈이 뭐야?\")\n",
        "\n",
        "# 구조화 프롬프트\n",
        "structured_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "너는 스마트시티 기술을 설명하는 과학 강사입니다.\n",
        "아래 개념을 초등학생이 이해할 수 있도록 간단히 설명하세요.\n",
        "출력은 3문장 이내로 요약해주세요.\n",
        "\n",
        "개념: 디지털 트윈\n",
        "\"\"\")\n",
        "\n",
        "# LCEL 문법으로 간결하게 실행\n",
        "basic_chain = basic_prompt | llm\n",
        "structured_chain = structured_prompt | llm \n",
        "\n",
        "# 결과 출력\n",
        "print(\"===== [단순 프롬프트 결과] =====\")\n",
        "print(basic_chain.invoke({}).content, \"\\n\")\n",
        "\n",
        "print(\"===== [구조화 프롬프트 결과] =====\")\n",
        "print(structured_chain.invoke({}).content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb5c2b32",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 56
        }
      },
      "source": [
        "결과 비교  \n",
        "- **기본 프롬프트**: 정의 위주, 다소 추상적 설명  \n",
        "- **구조화 프롬프트**: 역할·대상·형식이 명시되어 구체적이고 일관된 답변"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f49761b8",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 57
        }
      },
      "source": [
        "### 4.4 효과적인 프롬프트 설계의 원칙\n",
        "\n",
        "1. **명확성 (Clarity)**  \n",
        "   - 애매한 표현 대신 구체적 요청 사용  \n",
        "   - 예: \"요약해줘\" → \"3문장으로 핵심만 요약해줘\"\n",
        "\n",
        "2. **단계적 사고 유도 (Step-by-Step Reasoning)**  \n",
        "   - 복잡한 문제일수록 모델이 논리적 단계를 거치도록 유도  \n",
        "   - 예: \"답만 주지 말고, 풀이 과정을 단계별로 설명해줘.\"\n",
        "\n",
        "3. **제한조건 제시 (Constraints)**  \n",
        "   - 길이, 어조, 형식, 언어 등을 명시  \n",
        "   - 예: \"표 형식으로, 각 항목은 10단어 이내로.\"\n",
        "\n",
        "4. **예시 포함 (Few-Shot Prompting)**  \n",
        "   - 원하는 결과 예시를 포함해 모델이 패턴을 학습하도록 유도"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "704d725f",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 58
        }
      },
      "source": [
        "### 실습 2: 단계적 사고 유도 (Chain-of-Thought)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "eab1f005",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 59
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===== [Chain-of-Thought 결과] =====\n",
            "이 문제를 단계별로 해결해 보겠습니다.\n",
            "\n",
            "1. **문제 이해**: 한 상자에 사과가 12개 있습니다. 8상자가 있으니, 8상자에는 총 몇 개의 사과가 있을지를 구해야 합니다.\n",
            "\n",
            "2. **계산 방법**: 각 상자에 있는 사과의 수에 상자의 수를 곱해야 합니다. 즉, 한 상자에 있는 사과의 개수(12개)와 상자의 개수(8개)를 곱해주면 됩니다.\n",
            "\n",
            "3. **수식 설정**:\n",
            "   - 1상자에 있는 사과 개수 = 12개\n",
            "   - 상자 개수 = 8상자\n",
            "   - 총 사과 개수 = 12개 × 8상자\n",
            "\n",
            "4. **곱셈 계산**:\n",
            "   - 12 × 8 = 96\n",
            "\n",
            "5. **결과 정리**: 따라서 8상자에는 총 96개의 사과가 있습니다.\n",
            "\n",
            "정답: 96개\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "# 사용할 모델 지정.\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "\n",
        "# {problem} 자리에 실제 문제가 삽입\n",
        "prompt = PromptTemplate.from_template(\"\"\"\n",
        "아래 수학 문제를 단계별로 사고하여 풀이 과정을 설명하고,\n",
        "마지막 줄에 정답만 한 번 더 제시하세요.\n",
        "\n",
        "문제: {problem}\n",
        "\"\"\")\n",
        "\n",
        "# 파이프라인 형태로, 프롬프트 → 모델 순으로 연결\n",
        "chain = prompt | llm\n",
        "\n",
        "# invoke()는 입력 딕셔너리를 받아 한 번의 요청을 수행\n",
        "response = chain.invoke({\"problem\": \"한 상자에 사과가 12개 들어 있습니다. 8상자에는 사과가 몇 개인가요?\"})\n",
        "\n",
        "# 5️⃣ 결과 출력\n",
        "# response는 ChatMessage 객체이며, .content로 텍스트를 확인할 수 있습니다.\n",
        "print(\"===== [Chain-of-Thought 결과] =====\")\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6f45169",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 60
        }
      },
      "source": [
        "모델은 단순히 정답만 출력하지 않고, **사고 과정을 단계별로 풀어서 보여주는 방식**으로 응답하게 됩니다.  \n",
        "이 방식이 바로 **Chain-of-Thought(CoT)** 기법입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7643a708",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 61
        }
      },
      "source": [
        "### 4.5 프롬프트 개선 고급 기법\n",
        "\n",
        "| 기법 | 설명 | 예시 |\n",
        "|------|------|------|\n",
        "| **Self-Consistency Prompting** | 여러 답변을 생성하고 일관된 결론을 추출 | 복수 응답 후 \"가장 일관된 답변 선택\" |\n",
        "| **Few-Shot Prompting** | 원하는 형식의 예시를 제공하여 학습 효과 유도 | \"예시를 참고해 유사한 형태로 작성해줘.\" |\n",
        "| **Role Prompting** | 특정 전문가 역할을 설정해 답변 톤 통일 | \"너는 10년 경력의 변호사입니다.\" |\n",
        "| **System Prompt 활용** | 모델의 전반적 톤·방향을 사전 지정 | LangChain `SystemMessage` 활용 |\n",
        "\n",
        "**RAG(Retrieval-Augmented Generation)** 기법을 활용하면, 외부 문서나 데이터베이스에서 정보를 불러와 프롬프트에 포함시킬 수 있습니다.  \n",
        "RAG는 다음 수업에서 별도로 자세히 다룰 예정입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05f223c5",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 62
        }
      },
      "source": [
        "### 실습 3: Few-Shot 예시 포함 프롬프트"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "5d646efa",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 63
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===== [Few-Shot 프롬프트 결과] =====\n",
            "제품: 스마트 조명  \n",
            "카피: \"빛으로 분위기를, 하루를 특별하게!\"\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# 사용할 LLM 모델 정의\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "\n",
        "# Few-shot 프롬프트 작성\n",
        "# 예시(샘플 입력/출력 쌍)를 포함하여 모델이 유사한 스타일을 학습하도록 유도합니다.\n",
        "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "너는 친절한 제품 설명 카피라이터입니다.\n",
        "아래 예시를 참고해 유사한 문체로 작성하세요.\n",
        "\n",
        "[예시]\n",
        "제품: 무선 청소기\n",
        "카피: \"청소를 가볍게, 일상을 새롭게!\"\n",
        "\n",
        "제품: 노트북\n",
        "카피: \"언제나 일, 어디서나 일!\"\n",
        "\n",
        "[작성 요청]\n",
        "제품: {product}\n",
        "\"\"\")\n",
        "\n",
        "# LCEL 체인 구성\n",
        "# 파이프라인 형태로, (프롬프트 → 모델) 순서로 연결됩니다.\n",
        "chain = prompt | llm\n",
        "\n",
        "# chain.invoke() 실행\n",
        "# 프롬프트의 변수 {product} 자리에 실제 값을 전달합니다.\n",
        "response = chain.invoke({\"product\": \"스마트 조명\"})\n",
        "\n",
        "# 결과 출력\n",
        "# invoke()의 반환값은 ChatMessage 객체이며, .content로 텍스트를 추출할 수 있습니다.\n",
        "print(\"===== [Few-Shot 프롬프트 결과] =====\")\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59dcda75",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 64
        }
      },
      "source": [
        "모델은 주어진 예시의 **어조·길이·감정 톤을 모방**해 새로운 문장을 생성합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3be510e8",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 65
        }
      },
      "source": [
        "### 정리\n",
        "\n",
        "프롬프트 설계는 단순한 입력이 아니라, **LLM의 사고 구조를 설계하고 결과 품질을 제어하는 핵심 기술**입니다.  \n",
        "\n",
        "- 명확한 구조(역할·맥락·지시·형식)를 갖추면 일관된 출력 가능  \n",
        "- 단계적 사고(Chain-of-Thought)로 논리성과 신뢰성 강화  \n",
        "- 예시(Few-Shot)와 제한조건을 통해 결과 품질을 정밀 제어  \n",
        "- RAG는 외부 지식을 통합해 프롬프트를 확장하는 차세대 접근 방식으로 이어짐"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3366436",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 66
        }
      },
      "source": [
        "## 5. LangChain의 프롬프트 설계와 로컬 모델 활용"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80a2959f",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 67
        }
      },
      "source": [
        "### 5.1 PromptTemplate의 기본 개념\n",
        "\n",
        "LangChain에서 `PromptTemplate`은 **LLM에 전달할 프롬프트를 동적으로 구성하는 도구**입니다.  \n",
        "즉, \"고정된 문장 + 변수 값 삽입\" 형태로 입력값을 안전하게 관리해주는 일종의 **템플릿 엔진** 역할을 합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7d20032",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 68
        }
      },
      "source": [
        "### 주요 생성 방식 비교\n",
        "\n",
        "| 메서드 | 설명 | 예시 |\n",
        "|--------|------|------|\n",
        "| `from_template()` | 문자열 기반 템플릿 생성 (일반적) | `\"문제: {question}\"` |\n",
        "| `from_examples()` | 예시 데이터를 기반으로 few-shot 템플릿 구성 | 여러 예시 → 자동 포맷팅 |\n",
        "| `from_messages()` | System/User 메시지 구조를 명시하여 ChatPromptTemplate 생성 | 역할 기반 대화 구조 설계 |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a7c05e1",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 69
        }
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "template = \"안녕하세요 {name}님, 오늘 기분은 어떠신가요?\"\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "print(prompt.format(name=\"지훈\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "0eace7b3",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 70
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===== [from_examples Few-shot 결과] =====\n",
            "\"빛으로 감성을, 공간에 생명을 불어넣다.\"\n"
          ]
        }
      ],
      "source": [
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "\n",
        "# Few-shot 예시 데이터 정의\n",
        "examples = [\n",
        "    \"제품: 무선 청소기\\n카피: \\\"청소를 가볍게, 일상을 새롭게!\\\"\",\n",
        "    \"제품: 스마트워치\\n카피: \\\"시간을 넘어서, 건강을 관리하다.\\\"\",\n",
        "    \"제품: 전자책 리더기\\n카피: \\\"한 손에 도서관, 언제 어디서나 읽다.\\\"\"\n",
        "]\n",
        "\n",
        "# from_examples()로 프롬프트 템플릿 생성\n",
        "# - prefix: 프롬프트의 맨 앞에 들어갈 설명 문장\n",
        "# - suffix: 마지막에 실제 입력 변수가 들어갈 지시 문장\n",
        "prompt = PromptTemplate.from_examples(\n",
        "    examples=examples,\n",
        "    suffix=\"제품: {input}\\n카피:\",\n",
        "    input_variables=[\"input\"],\n",
        "    example_separator=\"\\n\\n\",\n",
        "    prefix=\"너는 감성적인 제품 카피라이터입니다.\\n아래 예시를 참고해 유사한 스타일의 문구를 작성하세요.\\n\"\n",
        ")\n",
        "\n",
        "# LCEL 기반 체인 구성\n",
        "chain = prompt | llm\n",
        "\n",
        "# 실행: 새로운 입력값 전달\n",
        "response = chain.invoke({\"input\": \"스마트 조명\"})\n",
        "\n",
        "# 결과 출력\n",
        "print(\"===== [from_examples Few-shot 결과] =====\")\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8048c6aa",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 71
        }
      },
      "source": [
        "ChatPromptTemplate의 구조적 프롬프트\n",
        "\n",
        "대화형 모델을 사용할 때는 `ChatPromptTemplate`을 활용합니다"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "b18b3e88",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 72
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "System: 너는 친절한 수학 선생님이야.\n",
            "Human: 다음 문제를 풀어줘: 3x + 2 = 11의 해를 구하시오\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "(\"system\", \"너는 친절한 수학 선생님이야.\"),\n",
        "(\"human\", \"다음 문제를 풀어줘: {question}\")\n",
        "])\n",
        "\n",
        "prompt_text = chat_prompt.format(question=\"3x + 2 = 11의 해를 구하시오\")\n",
        "print(prompt_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "944bd2aa",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 73
        }
      },
      "source": [
        "### 로컬 모델을 활용하여 사용해보기 :  Gemma-3-270M 활용 (2.7억)\n",
        "- 구글의 오픈소스 LLM으로, 수업에서 직접 실행 가능한 모델"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "48e58341",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 74
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n",
            "/tmp/ipykernel_585040/1714929602.py:28: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFacePipeline``.\n",
            "  llm = HuggingFacePipeline(pipeline=pipe)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===== [Gemma-3-270M Q&A 챗봇] =====\n",
            "질문: 인공지능과 머신러닝의 차이점을 간단히 설명해줘.\n",
            "답변: Human: \n",
            "당신은 친절하고 명확하게 답변하는 챗봇입니다.\n",
            "질문: 인공지능과 머신러닝의 차이점을 간단히 설명해줘.\n",
            "답변:\n",
            "인공지능(AI)은 컴퓨터가 스스로 학습하고 문제를 해결하는 능력을 갖추는 기술입니다.\n",
            "머신러닝(ML)은 컴퓨터가 스스로 학습하고 문제를 해결하는 기술입니다.\n",
            "인공지능과 머신러닝은 서로 다른 분야이지만, 각각의 장단점을 가지고 있습니다.\n",
            "자세한 내용은 아래에 설명해 드릴게요.\n",
            "**인공지능과 머신러닝의 차이점**\n",
            "**인공지능(AI)은 컴퓨터가 스스로 학습하고 문제를 해결하는 능력을 갖추는 기술입니다.**\n",
            "**머신러닝(ML)은 컴퓨터가 스스로 학습하고 문제를 해결하는 기술입니다.**\n",
            "**인공지능과 머신러닝은 서로 다른 분야이지만, 각각의 장단점을 가지고 있습니다.**\n",
            "**자세한 내용은 아래에 설명해 드릴게요.**\n",
            "**인공지능(AI)은 컴퓨터가 스스로 학습하고 문제를 해결하는 능\n"
          ]
        }
      ],
      "source": [
        "# Gemma-3-270M 모델을 활용한 LangChain Q&A 챗봇 예제\n",
        "# - Hugging Face Hub에서 모델을 불러와 LangChain으로 연결\n",
        "# - 간단한 질의응답을 수행하는 파이프라인 구성\n",
        "\n",
        "# 필요한 라이브러리 임포트\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# 모델 및 토크나이저 로드\n",
        "# Hugging Face 모델 허브에서 gemma-3-270m-it (instruction 튜닝 버전) 불러오기\n",
        "model_name = \"google/gemma-3-270m-it\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# Transformers의 text-generation 파이프라인 생성\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=200,   # 응답 길이 제한\n",
        "    temperature=0.7,      # 창의성 조절\n",
        "    do_sample=True\n",
        ")\n",
        "\n",
        "# 4️⃣ LangChain LLM 래퍼로 변환\n",
        "llm = HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "# 프롬프트 템플릿 정의\n",
        "# 사용자의 질문을 자연어로 받아 LLM에 전달합니다.\n",
        "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "당신은 친절하고 명확하게 답변하는 챗봇입니다.\n",
        "질문: {question}\n",
        "답변:\n",
        "\"\"\")\n",
        "\n",
        "# LCEL 문법으로 프롬프트와 모델 연결\n",
        "chain = prompt | llm\n",
        "\n",
        "# 예시 질의 실행\n",
        "question = \"인공지능과 머신러닝의 차이점을 간단히 설명해줘.\"\n",
        "response = chain.invoke({\"question\": question})\n",
        "\n",
        "# 결과 출력\n",
        "print(\"===== [Gemma-3-270M Q&A 챗봇] =====\")\n",
        "print(\"질문:\", question)\n",
        "print(\"답변:\", response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48002af2",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 75
        }
      },
      "source": [
        "##  6. 대화형 챗봇 (메모리)\n",
        "\n",
        "### Conversation Memory란?\n",
        "> **Conversation Memory(대화 메모리)** 는 LLM이 **이전 대화 내용을 기억하고 이어서 대답**할 수 있도록 하는 기능입니다.\n",
        "\n",
        "일반적인 LLM은 한 번의 요청(prompt)만 처리하고 끝나기 때문에,  \n",
        "사용자가 \"아까 말한 여행지 일정 다시 알려줘.\"처럼 과거 대화를 참조하면 맥락을 잃어버립니다.  \n",
        "→ 이때 필요한 것이 바로 **Memory(기억 기능)** 입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91a76157",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 76
        }
      },
      "source": [
        "### LangChain에서의 Memory 개념\n",
        "\n",
        "LangChain은 다양한 형태의 \"기억 클래스(memory classes)\"를 제공합니다.  \n",
        "대표적으로 아래 세 가지를 이해하면 충분합니다 👇\n",
        "\n",
        "| Memory 클래스 | 설명 | 특징 |\n",
        "|----------------|------|------|\n",
        "| **`ConversationBufferMemory`** | 단순히 대화 전체를 계속 저장 | 가장 기본적이고 직관적 |\n",
        "| **`ConversationBufferWindowMemory`** | 최근 N개의 대화만 저장 | \"단기 기억\" 형태로, 긴 대화를 효율적으로 유지 |\n",
        "| **`ConversationSummaryMemory`** | 이전 대화를 LLM이 요약해서 저장 | 긴 대화를 \"핵심 요약본\"으로 관리, 장기 기억에 적합 |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c91fc83a",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 77
        }
      },
      "source": [
        "### 프롬프트에 직접 넣는 방식 vs Memory 클래스 사용 비교\n",
        "\n",
        "| 비교 항목 | 프롬프트에 직접 삽입 | Memory 클래스 사용 |\n",
        "|------------|----------------------|---------------------|\n",
        "| 코드 복잡도 | ❌ 대화마다 프롬프트를 새로 생성해야 함 | ✅ LangChain이 자동으로 이전 대화 삽입 |\n",
        "| 확장성 | ❌ 대화가 길어지면 토큰 초과 문제 발생 | ✅ 오래된 대화는 요약/제한 가능 |\n",
        "| 유지보수 | ⚠️ 과거 대화 삽입 위치만 교체하면 되지만, 요약·토큰 제어를 직접 구현해야 함 | ✅ 메모리 객체만 관리하면 됨 |\n",
        "| 현실성 | ❌ \"일회용 챗봇\"에만 적합 | ✅ \"지속형 대화형 챗봇\" 구현 가능 |\n",
        "\n",
        "즉, **Memory는 LLM이 \"대화 히스토리를 스스로 관리\"하도록 하는 스마트한 도우미 클래스**입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d698018",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 78
        }
      },
      "source": [
        "### Memory 작동 원리\n",
        "1. 사용자가 LLM에 **질문(입력)** 을 보냅니다.  \n",
        "\n",
        "2. LLM이 **답변**을 생성하면, 이 대화 내용이 자동으로 **Memory에 저장**됩니다.  \n",
        "3. 사용자가 다음 질문을 입력하면,  \n",
        "   LangChain이 **이전 대화 기록을 자동으로 프롬프트에 포함시켜** LLM에 전달합니다.  \n",
        "4. LLM은 이 정보를 바탕으로 **맥락을 이해하고 연속적인 대화**를 생성합니다.\n",
        "\n",
        "→ 즉, 사용자는 별도로 과거 대화를 넣을 필요 없이, LangChain이 **\"대화 기록 → 프롬프트 삽입 → LLM 호출\"**  \n",
        "과정을 자동으로 처리합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37f59aec",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 79
        }
      },
      "source": [
        "### 코드 예시 1. 기본 Buffer Memory (대화 전체 저장)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "137ceb8e",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 80
        }
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'langchain.memory'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[26], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m### 코드 예시 ① 기본 Buffer Memory\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmemory\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ConversationBufferMemory\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompts\u001b[39;00m\u001b[38;5;250m  \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatPromptTemplate, MessagesPlaceholder\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# 모델 선언\u001b[39;00m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain.memory'"
          ]
        }
      ],
      "source": [
        "### 코드 예시 ① 기본 Buffer Memory\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_classic.memory import ConversationBufferMemory\n",
        "from langchain_core.prompts  import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "# 모델 선언\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "\n",
        "# 메모리 객체 생성 (대화 전체를 버퍼 형태로 저장)\n",
        "memory = ConversationBufferMemory(return_messages=True)\n",
        "\n",
        "# 프롬프트 템플릿\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"너는 여행 전문가야. 사용자의 질문에 친절하게 답해줘.\"),\n",
        "    MessagesPlaceholder(variable_name=\"history\"),  # 이전 대화 삽입 위치\n",
        "    (\"human\", \"{input}\")                           # 사용자 질문\n",
        "]) \n",
        "\n",
        "# LCEL 체인 구성\n",
        "chain = prompt | llm \n",
        "\n",
        "# 5️⃣ 연속 대화 시뮬레이션\n",
        "inputs = [\"부산 여행지 추천해줘.\", \"그럼 그 근처 맛집은 어디야?\"]\n",
        "\n",
        "for user_input in inputs:\n",
        "    # 메모리 불러오기\n",
        "    history = memory.load_memory_variables({})[\"history\"]\n",
        "    # 실행\n",
        "    result = chain.invoke({\"history\": history, \"input\": user_input})\n",
        "    # 결과 출력\n",
        "    print(f\"\\n사용자: {user_input}\\n 응답: {result.content}\")\n",
        "    # 메모리에 저장\n",
        "    memory.save_context({\"input\": user_input}, {\"output\": result.content})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "955aa51f",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 81
        }
      },
      "outputs": [],
      "source": [
        "memory.load_memory_variables({})['history']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ecf1d0e",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 82
        }
      },
      "source": [
        "### 코드 예시 2. 최근 대화만 유지 (Window Memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c75b20f",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 83
        }
      },
      "outputs": [],
      "source": [
        "### 코드 예시 ① 기본 Buffer Memory\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain_core.prompts  import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "# 모델 선언\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "\n",
        "# 메모리 객체 생성 (대화 전체를 버퍼 형태로 저장)\n",
        "memory = ConversationBufferWindowMemory(k=2, return_messages=True)\n",
        "\n",
        "# 프롬프트 템플릿\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"너는 여행 전문가야. 사용자의 질문에 친절하게 답해줘.\"),\n",
        "    MessagesPlaceholder(variable_name=\"history\"),  # 이전 대화 삽입 위치\n",
        "    (\"human\", \"{input}\")                           # 사용자 질문\n",
        "]) \n",
        "\n",
        "# LCEL 체인 구성\n",
        "chain = prompt | llm \n",
        "\n",
        "# 5️⃣ 연속 대화 시뮬레이션\n",
        "inputs = [\"부산 여행지 추천해줘.\", \"대한 민국의 수도는 어디야?\", \"서울의 인구는 몇명이야?\", \"내가 아까 추천해달라고 한 어행지는 어디야?\" ]\n",
        "\n",
        "for user_input in inputs:\n",
        "    # 메모리 불러오기\n",
        "    history = memory.load_memory_variables({})[\"history\"]\n",
        "    # 실행\n",
        "    result = chain.invoke({\"history\": history, \"input\": user_input})\n",
        "    # 결과 출력\n",
        "    print(f\"\\n사용자: {user_input}\\n 응답: {result.content}\")\n",
        "    # 메모리에 저장\n",
        "    memory.save_context({\"input\": user_input}, {\"output\": result.content})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6bcdd74",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 84
        }
      },
      "source": [
        "이 방식은 단기 기억처럼 작동합니다.  \n",
        "오래된 대화는 자동으로 지워져 토큰 낭비를 줄이고 효율성을 높입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03e995b3",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 85
        }
      },
      "source": [
        "### 코드 예시 3. 이전 대화를 요약해서 저장 (ConversationSummaryMemory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac68778a",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 86
        }
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_classic.memory import ConversationBufferMemory\n",
        "from langchain_core.prompts  import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "# 모델 선언\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n",
        "\n",
        "# 요약형 메모리 생성 — LLM이 과거 대화를 자동 요약하여 저장\n",
        "memory = ConversationSummaryMemory(llm=llm, return_messages=True)\n",
        "\n",
        "# 프롬프트 템플릿 정의\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"너는 여행 플래너야. 사용자의 요구에 따라 가족 여행 일정을 제안해줘.\"),\n",
        "    MessagesPlaceholder(variable_name=\"history\"),  # 요약된 과거 대화 자동 삽입\n",
        "    (\"human\", \"{input}\")                           # 현재 질문\n",
        "])\n",
        "\n",
        "# LCEL 체인 구성\n",
        "chain = prompt | llm\n",
        "\n",
        "# 연속 대화 시뮬레이션\n",
        "inputs = [\n",
        "    \"이번 주말에 가족 여행지 추천해줘.\",\n",
        "    \"지난번에 추천한 곳 중에 아이들이 놀기 좋은 곳은 어디였지?\",\n",
        "    \"그럼 거기 일정표를 하루만 짜줘.\"\n",
        "]\n",
        "\n",
        "for user_input in inputs:\n",
        "    history = memory.load_memory_variables({})[\"history\"]\n",
        "    result = chain.invoke({\"history\": history, \"input\": user_input})\n",
        "    print(f\"\\n👤 사용자: {user_input}\\n🤖 응답: {result.content}\")\n",
        "    memory.save_context({\"input\": user_input}, {\"output\": result.content})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81c4521f",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 87
        }
      },
      "outputs": [],
      "source": [
        "# 현재까지의 요약본 확인\n",
        "print(memory.buffer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7533a642",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 88
        }
      },
      "source": [
        "이 방식은 LLM이 이전 대화를 스스로 요약하여 \"장기 기억\" 처럼 보관합니다.  \n",
        "수백 문장의 대화도 핵심 내용만 남기기 때문에 효율적입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d167fda0",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 89
        }
      },
      "source": [
        "**핵심 정리**\n",
        "| Memory 타입 | 특징 | 사용 목적 |\n",
        "|--------------|--------|-------------|\n",
        "| **BufferMemory** | 모든 대화를 그대로 저장 | 짧은 대화, 디버깅용 |\n",
        "| **WindowMemory** | 최근 N개의 대화만 유지 | 실시간 채팅, 효율성 |\n",
        "| **SummaryMemory** | 이전 대화를 요약해서 유지 | 장기 기억, 맥락 유지형 챗봇 |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05931661",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 90
        }
      },
      "source": [
        "🔹 실습 : 아래 조건을 만족하는 \"나만의 대화형 여행 상담 챗봇\"을 만들어보세요.\n",
        "\n",
        "당신은 AI 여행 비서 트래블GPT 입니다.  \n",
        "고객이 여러 도시를 순서대로 여행하면서 맞춤 일정·숙소·음식을 요청할 때,  \n",
        "이전 대화 내용을 기억하며 연결된 제안을 해주는 지능형 여행 어시스턴트를 구현하세요.  \n",
        "\n",
        "1. ChatOpenAI(model=\"gpt-4o-mini\") 사용\n",
        "2. ConversationSummaryMemory로 장기 대화 기억 구현\n",
        "3. 답변에 반드시 \"이전 여행 내용을 바탕으로 추천드리면...\" 이라는 문구 포함\n",
        "  - 예시 : 이전 여행 내용을 바탕으로 추천드리면, 여수에서는 해상케이블카와 낭만포차거리를 꼭 가보세요.\n",
        "4. 대화 시나리오:\n",
        "  - 사용자가 \"부산 → 여수 → 강릉\" 순으로 도시를 이동\n",
        "  - 챗봇은 이전 도시에서 한 활동을 기억하고 \"연결된 여행 루트\"나 \"테마별 추천(가족/커플/힐링)\"을 제안할 것\n",
        "\n",
        "예시 시나리오\n",
        "```bash\n",
        "사용자: 이번 주말엔 부산 갈 건데 가족 여행지 좀 추천해줘.\n",
        "AI: 부산의 해운대, 아쿠아리움이 가족 단위로 인기예요!\n",
        "\n",
        "사용자: 이번엔 여수로 가볼까?\n",
        "AI: 이전 여행 내용을 바탕으로 추천드리면, 부산의 해변 감성에 이어 여수에서는 바다 전망 케이블카와 낭만포차를 즐기세요.\n",
        "\n",
        "사용자: 그럼 마지막은 강릉이 좋을까?\n",
        "AI: 이전 여행 내용을 바탕으로 추천드리면, 강릉에서는 여수보다 조용한 힐링 카페 거리와 바다 일출 코스를 권합니다.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "389aa786",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 91
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_585040/1492504287.py:9: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationSummaryMemory(llm=llm, return_messages=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " 사용자: 이번 주말엔 부산 갈 건데 가족 여행지 좀 추천해줘.\n",
            " 트래블GPT: 이전 여행 내용을 바탕으로 추천드리면, 부산에서 가족 여행에 적합한 장소는 다음과 같습니다:\n",
            "\n",
            "1. **해운대 해수욕장**: 넓은 백사장과 다양한 해양 스포츠를 즐길 수 있어 가족 모두가 즐길 수 있는 장소입니다.\n",
            "2. **부산 아쿠아리움**: 해운대 근처에 위치한 이 아쿠아리움에서는 다양한 해양 생물을 가까이에서 관찰할 수 있어 아이들에게 큰 인기를 끌고 있습니다.\n",
            "3. **부산 타워**: 광복동에 있는 부산 타워에서 도시의 전경을 감상하며 가족과 사진도 찍기 좋은 곳입니다.\n",
            "4. **감천문화마을**: 알록달록한 집들이 있는 이곳은 예쁜 사진을 찍기 좋은 장소이며, 마을을 탐방하면서 가족과 함께 즐거운 시간을 보낼 수 있습니다.\n",
            "5. **영도 태종대**: 아름다운 바다 경치를 감상할 수 있는 곳으로, 태종대 유람선을 타고 바다를 즐기는 것도 추천드립니다.\n",
            "\n",
            "부산에서 즐거운 가족 여행 되시길 바랍니다!\n",
            "\n",
            " 사용자: 이번엔 여수로 가볼까?\n",
            " 트래블GPT: 이전 여행 내용을 바탕으로 추천드리면, 여수에서 가족과 함께 즐길 수 있는 멋진 장소들이 많이 있습니다. \n",
            "\n",
            "1. **오동도**: 아름다운 경치와 함께 산책로가 잘 조성되어 있어 가족과 함께 여유롭게 걷기 좋은 곳입니다. 특히 봄에는 벚꽃이 만개해 더욱 아름답습니다.\n",
            "\n",
            "2. **여수 해상 케이블카**: 바다 위를 가로지르는 케이블카를 타고 멋진 경치를 감상할 수 있습니다. 아이들과 함께 특별한 경험이 될 거예요.\n",
            "\n",
            "3. **향일암**: 바다를 바라보며 조용한 시간을 보낼 수 있는 명소입니다. 해돋이와 해넘이를 감상하기 좋은 위치입니다.\n",
            "\n",
            "4. **여수 수산시장**: 신선한 해산물을 맛볼 수 있는 좋은 장소입니다. 지역의 특산물도 구입할 수 있어요.\n",
            "\n",
            "5. **돌산 공원**: 공원 내에 다양한 시설이 있어 가족 단위로 즐기기에 안성맞춤입니다. 특히 바다를 배경으로 한 사진 촬영에 적합합니다.\n",
            "\n",
            "여수에서 멋진 가족 여행을 즐기시길 바랍니다!\n",
            "\n",
            " 사용자: 그럼 마지막은 강릉이 좋을까?\n",
            " 트래블GPT: 이전 여행 내용을 바탕으로 추천드리면, 강릉도 가족 여행에 아주 좋은 선택입니다! 강릉에서는 경포대 해수욕장에서 해변을 즐기거나, 오죽헌을 방문하여 역사적인 탐방을 할 수 있습니다. 또한, 강릉 커피거리에서 맛있는 커피를 즐기고, 주문진 수산시장에서 신선한 해산물을 경험해보는 것도 좋습니다. 마지막으로, 정동진에서의 일출은 가족과 함께 특별한 순간을 만들기에 훌륭한 옵션입니다. 즐거운 강릉 여행 되시길 바랍니다!\n",
            "\n",
            " 요약된 Memory Buffer:\n",
            "The human asks the AI for family travel recommendations for a trip to Busan this weekend. The AI suggests several suitable locations, including Haeundae Beach for its sandy shores and water sports, the Busan Aquarium for observing marine life, Busan Tower for city views and photos, Gamcheon Culture Village for its colorful houses and exploration opportunities, and Yeongdo Taejongdae for beautiful ocean scenery and a boat ride, wishing the human a pleasant family trip. The human then considers a trip to Yeosu, and the AI provides recommendations for family-friendly locations in Yeosu, including Odongdo for scenic walks, the Yeosu Maritime Cable Car for stunning views, Hyangilam for peaceful ocean views and sunrises/sunsets, Yeosu Seafood Market for fresh seafood, and Dolsan Park for family activities and picturesque photo opportunities, wishing the human a wonderful family trip in Yeosu. Finally, the human inquires about Gangneung, and the AI recommends it as a great family travel option, highlighting activities such as enjoying the beach at Gyeongpodae, visiting Ojukheon for historical exploration, savoring coffee at the Gangneung Coffee Street, experiencing fresh seafood at Jumunjin Fish Market, and witnessing the sunrise at Jeongdongjin, wishing the human a joyful trip to Gangneung.\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_classic.memory import ConversationSummaryMemory\n",
        "from langchain_core.prompts  import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "# 모델 선언\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n",
        "\n",
        "# 메모리 생성 — 이전 대화를 요약하며 장기 기억\n",
        "memory = ConversationSummaryMemory(llm=llm, return_messages=True)\n",
        "\n",
        "# 프롬프트 템플릿 정의\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\",\n",
        "     \"너는 여행 비서 트래블GPT야. \"\n",
        "     \"사용자의 여행 루트를 기억하고, 이전 여행 내용을 바탕으로 다음 도시를 추천해줘. \"\n",
        "     \"답변에는 반드시 '이전 여행 내용을 바탕으로 추천드리면,' 이라는 문구를 포함해야 해.\"),\n",
        "    MessagesPlaceholder(variable_name=\"history\"),  # 과거 대화 자동 삽입\n",
        "    (\"human\", \"{input}\")                            # 현재 사용자 입력\n",
        "])\n",
        "\n",
        "# LCEL 체인 구성\n",
        "chain = prompt | llm\n",
        "\n",
        "# 대화 시나리오\n",
        "inputs = [\n",
        "    \"이번 주말엔 부산 갈 건데 가족 여행지 좀 추천해줘.\",\n",
        "    \"이번엔 여수로 가볼까?\",\n",
        "    \"그럼 마지막은 강릉이 좋을까?\"\n",
        "]\n",
        "\n",
        "# 연속 대화 시뮬레이션\n",
        "for user_input in inputs:\n",
        "    history = memory.load_memory_variables({})[\"history\"]\n",
        "    result = chain.invoke({\"history\": history, \"input\": user_input})\n",
        "    print(f\"\\n 사용자: {user_input}\\n 트래블GPT: {result.content}\")\n",
        "    memory.save_context({\"input\": user_input}, {\"output\": result.content})\n",
        "\n",
        "# 대화 요약 확인 (선택)\n",
        "print(\"\\n 요약된 Memory Buffer:\")\n",
        "print(memory.buffer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5af4f6d2",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 92
        }
      },
      "source": [
        "###  사용자 구분을 위한 session_id 사용\n",
        "\n",
        "LangChain의 기본 메모리(ConversationBufferMemory, ConversationBufferWindowMemory 등)는  \n",
        "내부적으로 여러 사용자를 자동으로 구분하는 기능(thread_id)을 제공하지 않습니다.  \n",
        "\n",
        "따라서 **멀티 사용자 환경에서는 각 사용자마다 독립된 Memory 인스턴스를 생성해야 하며**,  \n",
        "보통 `session_id` 또는 `user_id`와 같은 고유 식별자를 기준으로  \n",
        "\"사용자별 대화 히스토리\"를 분리하여 저장합니다.\n",
        "\n",
        "즉,  \n",
        "- A 사용자 → A만의 대화 메모리  \n",
        "- B 사용자 → B만의 대화 메모리  \n",
        "- 서로 대화 내용이 섞이지 않음  \n",
        "\n",
        "이 구조는 웹 서비스, 챗봇, 상담 시스템 등  \n",
        "여러 사용자가 동시에 접근하는 환경에서 필수적인 설계 방식입니다.\n",
        "\n",
        "아래 예시는 **대화 중간에 user_id가 바뀌더라도**,  \n",
        "해당 user_id의 메모리를 자동으로 불러와  \n",
        "사용자별로 독립된 대화 맥락이 유지되도록 구현한 코드입니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "d3989a03",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 93
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== User A 첫 대화 ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_585040/1415391925.py:10: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  user_memories[user_id] = ConversationBufferMemory(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "안녕하세요! 저는 감정을 느끼지 않지만, 여러분과 대화할 수 있어서 기쁩니다. 오늘은 어떤 이야기를 나누고 싶으신가요?\n",
            "\n",
            "=== User B 등장: completely separate memory ===\n",
            "죄송하지만, 실시간 날씨 정보를 제공할 수는 없습니다. 하지만 현재 위치를 알려주시면 일반적인 날씨 패턴이나 계절에 따른 날씨 정보를 제공해드릴 수 있습니다. 또는 날씨 앱이나 웹사이트를 통해 확인하실 수 있습니다. 도움이 필요하시면 말씀해 주세요!\n",
            "\n",
            "=== User A 다시 돌아옴 → 이전 문맥 유지 ===\n",
            "좋아요! 아까 어떤 이야기를 나누고 있었는지 말씀해 주시면 그 주제로 이어서 이야기해 볼게요. 어떤 내용이었나요?\n",
            "\n",
            "=== User B 다시 대화 → 자신의 문맥만 유지 ===\n",
            "물론입니다! 날씨에 대한 정보는 여러 가지 요소로 구성됩니다. 여기 몇 가지 주요 요소를 설명해 드릴게요:\n",
            "\n",
            "1. **온도**: 현재 기온과 체감 온도는 날씨의 중요한 요소입니다. 기온은 섭씨(°C) 또는 화씨(°F)로 표시됩니다.\n",
            "\n",
            "2. **습도**: 공기 중의 수분 함량을 나타내며, 상대 습도로 표현됩니다. 높은 습도는 더운 날씨에 불쾌감을 줄 수 있습니다.\n",
            "\n",
            "3. **강수량**: 비, 눈, 우박 등과 같은 강수의 양을 나타냅니다. 강수량은 보통 밀리미터(mm)로 측정됩니다.\n",
            "\n",
            "4. **바람**: 바람의 속도와 방향도 날씨에 큰 영향을 미칩니다. 바람이 강하면 체감 온도가 낮아질 수 있습니다.\n",
            "\n",
            "5. **기압**: 대기압은 날씨 변화에 중요한 역할을 합니다. 낮은 기압은 비와 구름을 동반할 가능성이 높고, 높은 기압은 맑은 날씨를 의미합니다.\n",
            "\n",
            "6. **구름**: 하늘의 구름 양과 종류도 날씨를 결정짓는 중요한 요소입니다. 구름이 많으면 비가 올 가능성이 높습니다.\n",
            "\n",
            "7. **기상 예보**: 기상청이나 날씨 앱을 통해 제공되는 예보는 앞으로의 날씨를 예측하는 데 도움을 줍니다.\n",
            "\n",
            "이 외에도 지역에 따라 특유의 기후나 날씨 패턴이 있을 수 있습니다. 특정 지역의 날씨에 대해 더 알고 싶으시면 말씀해 주세요!\n"
          ]
        }
      ],
      "source": [
        "from langchain_classic.memory import ConversationBufferMemory\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# 1) 사용자별 메모리 저장소\n",
        "user_memories = {}\n",
        "\n",
        "def get_memory(user_id):\n",
        "    \"\"\"user_id에 해당하는 메모리 반환, 없으면 새로 생성\"\"\"\n",
        "    if user_id not in user_memories:\n",
        "        user_memories[user_id] = ConversationBufferMemory(\n",
        "            return_messages=True,\n",
        "            memory_key=\"chat_history\"\n",
        "        )\n",
        "    return user_memories[user_id]\n",
        "\n",
        "\n",
        "# 2) LLM 설정\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2)\n",
        "\n",
        "\n",
        "# 3) 대화 함수\n",
        "def chat(user_id, user_message):\n",
        "    memory = get_memory(user_id)\n",
        "\n",
        "    # 메모리에 사용자 메시지 기록\n",
        "    memory.chat_memory.add_user_message(user_message)\n",
        "\n",
        "    # LLM 응답 생성\n",
        "    response = llm.invoke(\n",
        "        memory.load_memory_variables({})[\"chat_history\"]\n",
        "        + [{\"role\": \"user\", \"content\": user_message}]\n",
        "    )\n",
        "\n",
        "    # 메모리에 AI 응답 기록\n",
        "    memory.chat_memory.add_ai_message(response.content)\n",
        "\n",
        "    return response.content\n",
        "\n",
        "# 4) 테스트: 대화 도중 user_id가 변경됨----\n",
        "\n",
        "print(\"=== User A 첫 대화 ===\")\n",
        "print(chat(\"user_A\", \"안녕 AI야! 오늘 기분 어때?\"))\n",
        "\n",
        "print(\"\\n=== User B 등장: completely separate memory ===\")\n",
        "print(chat(\"user_B\", \"AI야, 나는 오늘 날씨가 궁금해.\"))\n",
        "\n",
        "print(\"\\n=== User A 다시 돌아옴 → 이전 문맥 유지 ===\")\n",
        "print(chat(\"user_A\", \"아까 이야기 이어서 하자.\"))\n",
        "\n",
        "print(\"\\n=== User B 다시 대화 → 자신의 문맥만 유지 ===\")\n",
        "print(chat(\"user_B\", \"아까 말한 날씨 관련해서 더 알려줘.\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bda1a332",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
          "cell_index": 94
        }
      },
      "source": [
        "### 정리\n",
        "\n",
        "이번 장에서는 **대규모 언어모델(LLM)의 개념과 발전 흐름**, 그리고 이를 활용하는 핵심 기술인  \n",
        "**프롬프트 엔지니어링(Prompt Engineering)** 의 원리와 실전 기법을 함께 살펴보았습니다.  \n",
        "\n",
        "ChatGPT나 Gemini 같은 **상용 LLM을 통해 LLM 기술이 산업 전반으로 확산되는 과정**을 살펴보았으며,  \n",
        "Llama·Qwen·Gemma 등 **오픈소스 모델들이 이를 기반으로 빠르게 발전하며 대체·보완 역할**을 하고 있음을 이해했습니다.  \n",
        "이를 통해 LLM이 단순한 대화형 AI를 넘어 **지식 자동화·문서 요약·코드 생성 등 다양한 산업 플랫폼**으로  \n",
        "확장되고 있음을 확인했습니다.  \n",
        "\n",
        "또한 프롬프트 설계만으로도 모델의 내부 파라미터를 바꾸지 않고  \n",
        "**사고 과정과 출력 품질을 제어하는 방법**을 실습을 통해 익혔습니다.  \n",
        "\n",
        "하지만 최근 LLM의 규모가 기하급수적으로 커지면서, 전체 파라미터를 모두 학습시키는 **완전한 파인튜닝(full fine-tuning)** 은  \n",
        "막대한 자원과 비용이 필요하다는 한계에 부딪혔습니다.  \n",
        "이에 따라 모델의 일부분만 효율적으로 조정하는 **경량 튜닝 기법**,  \n",
        "즉 **PEFT(Parameter-Efficient Fine-Tuning)** 이 새로운 대안으로 부상하고 있습니다.  \n",
        "\n",
        "특히 보안이 중요하거나 상용 LLM 사용이 제한된 기업·기관에서는  \n",
        "자체 구축한 오픈소스 LLM에 이러한 **프롬프트 엔지니어링 + 부분적 튜닝 전략**을 결합하여  \n",
        "내부 데이터를 안전하게 활용하려는 시도가 늘어나고 있습니다.  \n",
        "\n",
        "다음 장에서는 이러한 **PEFT의 개념과 주요 방식(LoRA, Prefix Tuning, Adapter 등)** 을 다루며,  \n",
        "프롬프트 설계 이후 단계에서 **모델을 실제 업무나 서비스 환경에 맞게 최적화하는 방법**을 배워보겠습니다.\n"
      ]
    }
  ],
  "metadata": {
    "encoded_email": [
      "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ=="
    ],
    "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg==",
    "inserted_date": [
      "2025-09-23"
    ],
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    },
    "watermark": {
      "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
      "inserted_date": "2026-01-19",
      "filename": "My00LuuMgOq3nOuqqCDslrjslrQg66qo6424KExMTSnqs7wg7ZSE66Gs7ZSE7Yq4IOyXlOyngOuLiOyWtOungS5pcHluYg=="
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}