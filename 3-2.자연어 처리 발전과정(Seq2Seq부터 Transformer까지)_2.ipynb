{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0fbfc894",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My0yLuyekOyXsOyWtCDsspjrpqwg67Cc7KCE6rO87KCVKFNlcTJTZXHrtoDthLAgVHJhbnNmb3JtZXLquYzsp4ApXzIuaXB5bmI=",
          "cell_index": 0
        }
      },
      "source": [
        "## 3. 트랜스포머 (Transformer)\n",
        "\n",
        "- Transformer 모델은 2017년 \"Attention Is All You Need\" 논문에서 소개된 혁신적인 딥러닝 아키텍처입니다.  \n",
        "이 모델은 자연어 처리(NLP) 분야에 큰 변화를 가져왔으며, 현재 대부분의 최신 언어 모델의 기반이 되고 있습니다.  \n",
        "\n",
        "- 이전의 RNN, LSTM과 같은 모델은 다음과 같은 문제점을 가지고 있었습니다.\n",
        "    - **순차적 처리**: 문장을 단어 단위로 순차적으로 처리하여 병렬화가 어려웠습니다.\n",
        "    - **장기 의존성 문제**: 긴 문장에서 멀리 떨어진 정보를 연결하는데 어려움이 있었습니다.\n",
        "    - **계산 복잡도**: 입력 시퀀스가 길어질수록 계산 시간이 기하급수적으로 증가했습니다.\n",
        "\n",
        "- Transformer 모델은 병렬 처리가 가능한 Self-Attention 메커니즘을 중심으로 설계되었으며, 기존 모델의 제약을 극복하는 데 초점을 맞추고 있습니다.\n",
        "  - **병렬 처리**: Self-Attention 메커니즘을 통해 전체 입력을 동시에 처리합니다.\n",
        "  - **장거리 의존성 포착**: 문장 내 모든 단어 간의 관계를 직접적으로 모델링합니다.\n",
        "  - **확장성**: 대규모 데이터셋에서도 효율적으로 학습이 가능합니다.\n",
        "\n",
        "* 논문 링크: [Attention is All You Need](https://arxiv.org/abs/1706.03762)\n",
        "* 참고: https://wikidocs.net/31379\n",
        "\n",
        "  <img src=\"https://images.velog.io/images/jekim5418/post/79d1b604-b9eb-47a2-b6b1-0e3d158f955d/Transformer%20architecture.png\" width=\"400\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b7130df",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My0yLuyekOyXsOyWtCDsspjrpqwg67Cc7KCE6rO87KCVKFNlcTJTZXHrtoDthLAgVHJhbnNmb3JtZXLquYzsp4ApXzIuaXB5bmI=",
          "cell_index": 1
        }
      },
      "source": [
        "트랜스포머는 기계번역 작업에서 더 적은 연산량으로 당시 최고 수준을 달성했습니다.  \n",
        "어떻게 이러한 혁신이 가능했는지 차근차근 알아보겠습니다.\n",
        "\n",
        "<img src=\"image/transformer_table.png\" width=\"600\"> "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c252ae1",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My0yLuyekOyXsOyWtCDsspjrpqwg67Cc7KCE6rO87KCVKFNlcTJTZXHrtoDthLAgVHJhbnNmb3JtZXLquYzsp4ApXzIuaXB5bmI=",
          "cell_index": 2
        }
      },
      "source": [
        "### 3.1 핵심 개념 : Self-Attention\n",
        "\n",
        "Transformer의 전체 구조를 살펴보기 전에, 먼저 **가장 중요한 핵심 메커니즘인 Self-Attention**을 가볍게 살펴보겠습니다.\n",
        "\n",
        "Self-Attention은 Transformer가 **문맥을 이해하는 방식의 핵심**입니다.  \n",
        "입력에서 출력으로 이어지는 전체 흐름을 보기 전에,  \n",
        "“Transformer는 문장을 어떻게 이해하려고 하는가?”라는 관점에서 먼저 접근해봅니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4674606",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My0yLuyekOyXsOyWtCDsspjrpqwg67Cc7KCE6rO87KCVKFNlcTJTZXHrtoDthLAgVHJhbnNmb3JtZXLquYzsp4ApXzIuaXB5bmI=",
          "cell_index": 3
        }
      },
      "source": [
        "#### **Self-Attention: 문맥 이해의 핵심**\n",
        "\n",
        "Self-Attention은 입력 시퀀스 안에서 **각 토큰이 다른 모든 토큰과 얼마나 관련이 있는지**를 계산하는 메커니즘입니다.\n",
        "\n",
        "즉, 모델은 각 단어를 처리할 때  \n",
        "> “이 단어를 이해하기 위해 문장 안의 어떤 단어를 얼마나 참고해야 할까?”  \n",
        "를 스스로 판단합니다.\n",
        "\n",
        "Self-Attention은 다음과 같은 절차로 동작합니다.\n",
        "\n",
        "1. 각 입력 토큰으로부터 **Query, Key, Value 벡터**를 생성합니다.\n",
        "2. Query와 모든 Key의 내적을 계산하여 **토큰 간의 연관성 점수**를 구합니다.\n",
        "3. 이 점수를 Value에 가중합하여 **문맥이 반영된 새로운 표현**을 만듭니다.\n",
        "\n",
        "이 과정은 문장 내 모든 토큰에 대해 **동시에(parallel)** 수행됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bffa1d6",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My0yLuyekOyXsOyWtCDsspjrpqwg67Cc7KCE6rO87KCVKFNlcTJTZXHrtoDthLAgVHJhbnNmb3JtZXLquYzsp4ApXzIuaXB5bmI=",
          "cell_index": 4
        }
      },
      "source": [
        "#### Self-Attention이 만들어내는 문맥 이해\n",
        "\n",
        "예를 들어 다음 문장을 보겠습니다.\n",
        "\n",
        "> **“길에서 바나나를 먹는 원숭이를 보았다.  \n",
        "> 그것은 맛있어 보였다.”**\n",
        "\n",
        "사람은 자연스럽게  \n",
        "**‘그것은’이 ‘바나나’를 가리킨다**는 것을 이해합니다.\n",
        "\n",
        "하지만 모델의 입장에서는 `그것은`이  \n",
        "- 길을 가리키는지,\n",
        "- 원숭이를 가리키는지,\n",
        "- 바나나를 가리키는지\n",
        "\n",
        "를 단순한 순차 처리만으로 판단하기는 쉽지 않습니다.\n",
        "\n",
        "Transformer는 Self-Attention을 통해  \n",
        "문장 안의 모든 단어 토큰 간의 연관성을 한 번에 계산하고,  \n",
        "그중 **가장 중요한 연결 관계에 집중**할 수 있게 됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bea3c4ac",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My0yLuyekOyXsOyWtCDsspjrpqwg67Cc7KCE6rO87KCVKFNlcTJTZXHrtoDthLAgVHJhbnNmb3JtZXLquYzsp4ApXzIuaXB5bmI=",
          "cell_index": 5
        }
      },
      "source": [
        "그 결과, `그것은`이 ‘바나나’와 가장 강하게 연결되어 있다는 점을  \n",
        "모델 스스로 학습할 수 있습니다.\n",
        "\n",
        "Self-Attention은 이처럼 문장을 왼쪽에서 오른쪽으로 단순히 읽는 것이 아니라,  \n",
        "**문장 전체를 동시에 바라보며 중요한 관계를 찾아내는 방식**으로  \n",
        "문맥을 이해하도록 만들어줍니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08007196",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My0yLuyekOyXsOyWtCDsspjrpqwg67Cc7KCE6rO87KCVKFNlcTJTZXHrtoDthLAgVHJhbnNmb3JtZXLquYzsp4ApXzIuaXB5bmI=",
          "cell_index": 6
        }
      },
      "source": [
        "### 3.2 핵심 개념 : **병렬 처리** (효율성의 비결)\n",
        "\n",
        "<img src=\"https://ars.els-cdn.com/content/image/1-s2.0-S0926580521000595-gr4.jpg\">\n",
        "\n",
        "Transformer의 또 다른 핵심 특징은 **GPU와 같은 병렬 처리 하드웨어를 최대한 활용할 수 있다는 점**입니다.\n",
        "\n",
        "이는 단순히 “연산이 빠르다”는 의미를 넘어,  \n",
        "대규모 데이터셋과 대형 모델을 **현실적인 시간 안에 학습 가능하게 만든 핵심 요인**입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09a4e4fd",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My0yLuyekOyXsOyWtCDsspjrpqwg67Cc7KCE6rO87KCVKFNlcTJTZXHrtoDthLAgVHJhbnNmb3JtZXLquYzsp4ApXzIuaXB5bmI=",
          "cell_index": 7
        }
      },
      "source": [
        "#### 복습 : 왜 기존 모델은 병렬 처리가 어려웠을까?\n",
        "\n",
        "RNN이나 LSTM과 같은 순환 신경망은 **이전 시점의 계산 결과가 다음 시점의 입력에 직접적으로 의존**합니다.\n",
        "\n",
        "즉,\n",
        "\n",
        "- 첫 번째 토큰을 처리해야\n",
        "- 두 번째 토큰을 처리할 수 있고\n",
        "- 그 결과가 있어야 세 번째 토큰으로 넘어갈 수 있습니다.\n",
        "\n",
        "이러한 구조적 특성 때문에 토큰 단위의 연산을 **순차적으로 처리할 수밖에 없으며**,  \n",
        "GPU의 병렬 연산 능력을 충분히 활용하기 어렵습니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7d1492a",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My0yLuyekOyXsOyWtCDsspjrpqwg67Cc7KCE6rO87KCVKFNlcTJTZXHrtoDthLAgVHJhbnNmb3JtZXLquYzsp4ApXzIuaXB5bmI=",
          "cell_index": 8
        }
      },
      "source": [
        "### Transformer는 무엇이 다를까?\n",
        "\n",
        "Transformer는 Self-Attention 구조를 통해 **문장 전체의 모든 토큰을 한 번에 처리**합니다.\n",
        "\n",
        "각 토큰은 서로를 참고하지만,  \n",
        "계산 자체는 **이전 토큰의 결과를 기다릴 필요 없이 동시에 수행**됩니다.\n",
        "\n",
        "그 결과,\n",
        "\n",
        "- 모든 토큰의 Attention 계산\n",
        "- 모든 벡터 변환 연산\n",
        "\n",
        "을 GPU 상에서 **병렬로 처리할 수 있습니다.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8a16a81",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My0yLuyekOyXsOyWtCDsspjrpqwg67Cc7KCE6rO87KCVKFNlcTJTZXHrtoDthLAgVHJhbnNmb3JtZXLquYzsp4ApXzIuaXB5bmI=",
          "cell_index": 9
        }
      },
      "source": [
        "#### 비유: 공장 생산 라인\n",
        "\n",
        "이 차이를 공장 생산 과정에 비유할 수 있습니다.\n",
        "\n",
        "- **RNN / LSTM**  \n",
        "  하나의 생산 라인에서 제품을 **앞 단계가 끝날 때까지 기다리며** 순차적으로 조립하는 방식\n",
        "\n",
        "- **Transformer**  \n",
        "  여러 생산 라인에서 부품을 **동시에 생산한 뒤**, 마지막에 조립하는 방식\n",
        "\n",
        "Transformer는 이 구조 덕분에 생산량(처리 속도)을 획기적으로 늘릴 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad0242f8",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My0yLuyekOyXsOyWtCDsspjrpqwg67Cc7KCE6rO87KCVKFNlcTJTZXHrtoDthLAgVHJhbnNmb3JtZXLquYzsp4ApXzIuaXB5bmI=",
          "cell_index": 10
        }
      },
      "source": [
        "#### 병렬 처리가 가져온 변화\n",
        "\n",
        "이러한 병렬 처리 가능성 덕분에 Transformer는\n",
        "\n",
        "- 학습 속도의 대폭 향상\n",
        "- 대규모 데이터셋 학습 가능\n",
        "- 모델 크기 확장(수억~수천억 파라미터)\n",
        "\n",
        "을 가능하게 만들었고, 이는 오늘날 대형 언어 모델(LLM)의 기반이 되었습니다.\n",
        "\n",
        "즉, Transformer의 성공은  \n",
        "**모델 구조의 성능뿐 아니라, 하드웨어 친화적인 설계**에서 비롯되었다고 볼 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d48320d",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My0yLuyekOyXsOyWtCDsspjrpqwg67Cc7KCE6rO87KCVKFNlcTJTZXHrtoDthLAgVHJhbnNmb3JtZXLquYzsp4ApXzIuaXB5bmI=",
          "cell_index": 11
        }
      },
      "source": [
        "### 3.3 구성 요소\n",
        "\n",
        "이제부터 입력부터 출력까지 트랜스포머의 모든 구성요소들에 대해 자세히 리뷰해보겠습니다. \n",
        "\n",
        "> 아래 요소들은 **인코더와 디코더가 공통으로 사용하는 기본 블록**입니다.  \n",
        "> 디코더는 여기에 마스크드 Self-Attention과 인코더-디코더 Attention이 추가됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0089f1cc",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My0yLuyekOyXsOyWtCDsspjrpqwg67Cc7KCE6rO87KCVKFNlcTJTZXHrtoDthLAgVHJhbnNmb3JtZXLquYzsp4ApXzIuaXB5bmI=",
          "cell_index": 12
        }
      },
      "source": [
        "#### **입력 임베딩 (Input Embedding)**\n",
        "\n",
        "- **역할**: 텍스트 입력(단어 또는 토큰)을 모델이 처리할 수 있는 **연속적인 벡터 공간**으로 변환\n",
        "- **의도**: 이산적인 토큰 ID를 **의미 정보를 담은 실수 벡터**로 표현하기 위함"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd73d743",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My0yLuyekOyXsOyWtCDsspjrpqwg67Cc7KCE6rO87KCVKFNlcTJTZXHrtoDthLAgVHJhbnNmb3JtZXLquYzsp4ApXzIuaXB5bmI=",
          "cell_index": 13
        }
      },
      "source": [
        "##### 토큰 → 벡터 변환은 어떻게 이루어질까?\n",
        "\n",
        "Transformer에서 입력 임베딩은  \n",
        "**임베딩 행렬(Embedding Matrix)** 을 이용한 **룩업 테이블(lookup table) 방식**으로 수행됩니다.\n",
        "\n",
        "- 전체 어휘 크기를 `V`\n",
        "- 임베딩 차원을 `d_model`이라 하면  \n",
        "- 임베딩 행렬의 크기는 `V × d_model`\n",
        "\n",
        "각 토큰은 이 행렬에서 **자신의 ID에 해당하는 한 행(row)** 을 가져옵니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ac2952a",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My0yLuyekOyXsOyWtCDsspjrpqwg67Cc7KCE6rO87KCVKFNlcTJTZXHrtoDthLAgVHJhbnNmb3JtZXLquYzsp4ApXzIuaXB5bmI=",
          "cell_index": 14
        }
      },
      "source": [
        "##### 예시\n",
        "\n",
        "> **“고양이는 귀엽다”**\n",
        "\n",
        "1. 토큰화 후 각 단어를 고유한 ID로 변환  \n",
        "   - 고양이 → 1  \n",
        "   - 는 → 2  \n",
        "   - 귀엽다 → 3  \n",
        "\n",
        "2. 임베딩 행렬에서 해당 ID의 행을 조회  \n",
        "   - 고양이 → `E[1] = [0.1, 0.3, -0.2, ...]`\n",
        "   - 는 → `E[2] = [0.5, -0.1, 0.4, ...]`\n",
        "   - 귀엽다 → `E[3] = [-0.2, 0.7, 0.1, ...]`\n",
        "\n",
        "이렇게 얻어진 벡터들이 Transformer의 실제 입력이 됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2be549f",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My0yLuyekOyXsOyWtCDsspjrpqwg67Cc7KCE6rO87KCVKFNlcTJTZXHrtoDthLAgVHJhbnNmb3JtZXLquYzsp4ApXzIuaXB5bmI=",
          "cell_index": 15
        }
      },
      "source": [
        "##### 중요한 점\n",
        "\n",
        "- 이 임베딩 벡터들은 **학습 과정에서 함께 업데이트되는 파라미터**입니다.\n",
        "- 즉, 임베딩은 고정된 규칙이 아니라 **모델이 데이터로부터 의미를 학습해 나가는 표현 공간**입니다.\n",
        "- 의미적으로 비슷한 단어들은 학습이 진행될수록 **벡터 공간에서도 가까워지게 됩니다.**\n",
        "\n",
        "입력 임베딩은 이후 단계에서  \n",
        "**위치 정보(Positional Encoding)** 와 결합되어,  \n",
        "Transformer가 문장의 구조와 순서를 함께 이해할 수 있도록 합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "733673df",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My0yLuyekOyXsOyWtCDsspjrpqwg67Cc7KCE6rO87KCVKFNlcTJTZXHrtoDthLAgVHJhbnNmb3JtZXLquYzsp4ApXzIuaXB5bmI=",
          "cell_index": 16
        }
      },
      "source": [
        "#### **포지셔널 인코딩(Positional Encoding):**\n",
        "\n",
        "* 챗봇, 번역기 등에서 어순은 굉장히 중요함. 어순에 따라 의미가 완전히 반대가 될 수 있음\n",
        "  - 예시 : 사람이 개를 쫓는다 vs 개를 사람이 쫓는다\n",
        "* 기존의 RNN은 단어의 위치를 따라 순차적으로 입력받아 단어의 위치정보를 활용할 수 있었음\n",
        "* 트랜스포머의 경우, RNN을 활용하지 않았기 때문에 단어의 위치정보를 다른 방식으로 줄 필요가 있음\n",
        "* 이를 위해 **각 단어의 임베딩 벡터에 위치 정보들을 더하게 되는데** 이를 포지셔널 인코딩이라 함\n",
        "* 보통 포지셔널 인코딩은 sin, cos을 이용하여 계산\n",
        "\n",
        "<img src=\"https://kazemnejad.com/img/transformer_architecture_positional_encoding/model_arc.jpg\" width=\"700\">\n",
        "\n",
        "- 역할: 시퀀스에서 각 단어의 위치 정보를 제공.\n",
        "- 의도: Transformer가 순서를 알 수 있도록 보완.\n",
        "- 동작 방식: 사인(sin)과 코사인(cos) 함수로 생성된 고정 위치 정보 추가.\n",
        "- 수학적 표현\n",
        "  - $ PE(pos, 2i) = sin(pos / 10000^{2i/d_{model}}) $\n",
        "  - $ PE(pos, 2i+1) = cos(pos / 10000^{2i/d_{model}}) $\n",
        "  - 여기서 *pos*는 단어의 위치, *i*는 차원을 나타냅니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "948383c9",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My0yLuyekOyXsOyWtCDsspjrpqwg67Cc7KCE6rO87KCVKFNlcTJTZXHrtoDthLAgVHJhbnNmb3JtZXLquYzsp4ApXzIuaXB5bmI=",
          "cell_index": 17
        }
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    PyTorch 버전 포지셔널 인코딩 모듈.\n",
        "    입력 텐서 shape: (batch_size, seq_len, d_model)\n",
        "    \"\"\"\n",
        "    def __init__(self, max_seq_len: int, d_model: int):\n",
        "        super().__init__()\n",
        "\n",
        "        # pos: (max_seq_len, 1)\n",
        "        position = torch.arange(0, max_seq_len, dtype=torch.float32).unsqueeze(1)\n",
        "        # div_term: (d_model/2,)\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, d_model, 2, dtype=torch.float32)\n",
        "            * (-math.log(10000.0) / d_model)\n",
        "        )\n",
        "\n",
        "        # pe: (max_seq_len, d_model)\n",
        "        pe = torch.zeros(max_seq_len, d_model, dtype=torch.float32)\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)  # 짝수 인덱스\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)  # 홀수 인덱스\n",
        "\n",
        "        # 배치 차원 추가: (1, max_seq_len, d_model)\n",
        "        pe = pe.unsqueeze(0)\n",
        "\n",
        "        # 학습 파라미터는 아니지만, 모델과 함께 GPU로 이동되도록 buffer로 등록\n",
        "        self.register_buffer(\"pos_encoding\", pe)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        x: (batch_size, seq_len, d_model)\n",
        "        \"\"\"\n",
        "        seq_len = x.size(1)\n",
        "        # 앞 seq_len 만큼의 포지셔널 인코딩을 더해줌\n",
        "        return x + self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "\n",
        "# 샘플 포지셔널 인코딩 시각화\n",
        "sample_pos_encoding = PositionalEncoding(50, 128)  # 위치 크기 50, 임베딩 차원 128\n",
        "pe_np = sample_pos_encoding.pos_encoding[0].cpu().numpy()  # (50, 128)\n",
        "\n",
        "plt.pcolormesh(pe_np, cmap='RdBu')\n",
        "plt.xlabel('Depth')        # x축: 임베딩 차원\n",
        "plt.xlim((0, 128))\n",
        "plt.ylabel('Position')     # y축: 위치\n",
        "plt.colorbar()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7ff4819",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My0yLuyekOyXsOyWtCDsspjrpqwg67Cc7KCE6rO87KCVKFNlcTJTZXHrtoDthLAgVHJhbnNmb3JtZXLquYzsp4ApXzIuaXB5bmI=",
          "cell_index": 18
        }
      },
      "source": [
        "### **Self Attention:**\n",
        "\n",
        "Seq2Seq의 어텐션은 입력 시퀀스와 출력 시퀀스 간의 어텐션이었다면, 트랜스포머의 셀프 어텐션은 입력 문장 내에서 단어간 관계성을 파악하는 과정입니다.\n",
        "\n",
        "<img src=\"image/selfattention.png\">\n",
        "\n",
        "이미지 출처 : https://www.researchgate.net/figure/An-example-of-the-self-attention-mechanism-following-long-distance-dependency-in-the_fig1_350714675"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a0e8142",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My0yLuyekOyXsOyWtCDsspjrpqwg67Cc7KCE6rO87KCVKFNlcTJTZXHrtoDthLAgVHJhbnNmb3JtZXLquYzsp4ApXzIuaXB5bmI=",
          "cell_index": 19
        }
      },
      "source": [
        "### **Multi-Head Attention:**\n",
        "\n",
        "\n",
        "- 역할: Self-Attention을 병렬적으로 여러 관점(또는 하위 공간)에서 계산하여 더 풍부하고 다양한 문맥 정보를 추출\n",
        "- 의도: 문장 내 토큰 간의 여러 관계를 동시에 학습하여 복잡한 의존성을 효과적으로 모델링\n",
        "- 목적: 문맥 정보와 의존성을 다각도로 포착해 더 정확한 표현 학습을 가능하게 함.\n",
        "- 동작 방식:\n",
        "\n",
        "  <img src = \"image/mha_img_original.png\" width=\"300\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65e72899",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My0yLuyekOyXsOyWtCDsspjrpqwg67Cc7KCE6rO87KCVKFNlcTJTZXHrtoDthLAgVHJhbnNmb3JtZXLquYzsp4ApXzIuaXB5bmI=",
          "cell_index": 20
        }
      },
      "source": [
        "입력 벡터를 Query, Key, Value로 선형 변환 (세 가지 행렬 곱)\n",
        "\n",
        "<img src = \"image/key-query-value.png\" width=\"600\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a67c72b",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My0yLuyekOyXsOyWtCDsspjrpqwg67Cc7KCE6rO87KCVKFNlcTJTZXHrtoDthLAgVHJhbnNmb3JtZXLquYzsp4ApXzIuaXB5bmI=",
          "cell_index": 21
        }
      },
      "source": [
        "각 Head에서 독립적으로 Self-Attention 계산 (스케일 조정된 점곱 연산 포함)\n",
        "\n",
        "<img src = \"image/multiheadattention.png\" width=\"500\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c6b3d50",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My0yLuyekOyXsOyWtCDsspjrpqwg67Cc7KCE6rO87KCVKFNlcTJTZXHrtoDthLAgVHJhbnNmb3JtZXLquYzsp4ApXzIuaXB5bmI=",
          "cell_index": 22
        }
      },
      "source": [
        "각 Head의 출력을 Concatenate한 뒤 다시 선형 변환으로 결합.\n",
        "\n",
        "<img src = \"image/transformer_attention.png\" width=\"500\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bdffb721",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My0yLuyekOyXsOyWtCDsspjrpqwg67Cc7KCE6rO87KCVKFNlcTJTZXHrtoDthLAgVHJhbnNmb3JtZXLquYzsp4ApXzIuaXB5bmI=",
          "cell_index": 23
        }
      },
      "source": [
        "Attention을 통해 학습된 문맥적으로 강화된 임베딩\n",
        "\n",
        "<img src=\"image/concat.png\" width=\"400\">\n",
        "\n",
        "이미지출처 : https://wikidocs.net/31379\n",
        "\n",
        "결과 : 입력 문장의 각 토큰 간 상관관계를 도출해 문맥과 의미를 보다 풍부하게 표현 가능\n",
        "\n",
        "비유: 다각도로 문장 분석하기\n",
        "- **문법 전문가**: 문장 구조에 집중.\n",
        "- **의미론 전문가**: 단어의 의미 관계에 집중.\n",
        "- **문맥 전문가**: 전체적인 맥락에 집중.\n",
        "\n",
        "인코더는 **Self-Attention 블록**만 쌓아 입력 문장을 인코딩하고,  \n",
        "디코더는 거기에 **마스크드 Self-Attention + 인코더-디코더 Attention**을 추가하여 다음 단어를 생성합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b7d7fe4",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My0yLuyekOyXsOyWtCDsspjrpqwg67Cc7KCE6rO87KCVKFNlcTJTZXHrtoDthLAgVHJhbnNmb3JtZXLquYzsp4ApXzIuaXB5bmI=",
          "cell_index": 24
        }
      },
      "source": [
        "### **Residual Connection(Add):**\n",
        "Residual Connection은 각 하위 계층(Sublayer)의 출력을 입력과 더하는 구조입니다.  \n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*UMHrHYPxQ6KmN89dutvyCQ.png\" width=\"500\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ae3867c",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My0yLuyekOyXsOyWtCDsspjrpqwg67Cc7KCE6rO87KCVKFNlcTJTZXHrtoDthLAgVHJhbnNmb3JtZXLquYzsp4ApXzIuaXB5bmI=",
          "cell_index": 25
        }
      },
      "source": [
        "**수식 표현**:\n",
        "\n",
        "$\\text{Output} = \\text{LayerNorm}(x + \\text{Sublayer}(x))$\n",
        "  \n",
        "  여기서:\n",
        "  - $ x $: 하위 계층에 입력된 값.\n",
        "  - $\\text{Sublayer}(x)$: Self-Attention 또는 FFNN 층의 출력.\n",
        "  - $\\text{LayerNorm}$: 신경망의 각 토큰 벡터(feature 차원) 내에서 평균을 0, 분산을 1로 정규화하여  \n",
        "값의 분포를 안정화하고 학습을 안정적으로 유지하도록 하는 Layer Normalization\n",
        "\n",
        "- **의도**:\n",
        "  - 입력과 출력의 정보를 결합하여 모델이 더 풍부한 표현을 학습할 수 있도록 함.\n",
        "  - 네트워크가 깊어질수록 발생하는 기울기 소실 문제를 완화.\n",
        "\n",
        "이를 통해 정보 손실을 방지하고 학습을 안정화할 수 있습니다.\n",
        "- 역할: 입력과 출력의 합을 계산하여 기울기 소실 문제 완화.\n",
        "- 목적: 정보 손실 방지 및 학습 안정성 제공."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f66ed00e",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My0yLuyekOyXsOyWtCDsspjrpqwg67Cc7KCE6rO87KCVKFNlcTJTZXHrtoDthLAgVHJhbnNmb3JtZXLquYzsp4ApXzIuaXB5bmI=",
          "cell_index": 26
        }
      },
      "source": [
        "### **Layer Normalization:**\n",
        "Layer Normalization은 각 계층의 출력을 정규화하여 학습 과정을 안정화합니다.\n",
        "- **수식 표현**:\n",
        "\n",
        "$$\n",
        "y = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} \\cdot \\gamma + \\beta\n",
        "$$\n",
        "\n",
        "  여기서:\n",
        "  - $x$: 한 샘플(또는 토큰)의 feature 벡터\n",
        "  - $\\\\mu $: 해당 샘플의 feature 평균\n",
        "  - $\\\\sigma$: 해당 샘플의 feature 표준편차\n",
        "  - $\\\\epsilon$: 매우 작은 값 (0으로 나누는 것을 방지)\n",
        "  - $\\\\gamma$, $\\beta $: 학습 가능한 스케일(scale) 및 시프트(shift) 파라미터\n",
        "    - 정규화 과정에서 모델의 표현력이 떨어질 수 있기 때문에 γ(감마)와 β(베타)가 다시 그 표현력을 복원\n",
        "\n",
        "- **작동 방식**:\n",
        "  - 각 계층에서 입력 데이터를 정규화하여 평균과 분산을 일정하게 유지.\n",
        "  - 학습 속도를 높이고, 불안정한 변화(Gradient Explosion/Vanishing)를 방지.\n",
        "\n",
        "- 역할: 각 층의 출력을 정규화하여 학습 안정성 향상.\n",
        "- 의도: 학습 과정에서의 성능 저하 방지."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54838b5e",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My0yLuyekOyXsOyWtCDsspjrpqwg67Cc7KCE6rO87KCVKFNlcTJTZXHrtoDthLAgVHJhbnNmb3JtZXLquYzsp4ApXzIuaXB5bmI=",
          "cell_index": 27
        }
      },
      "source": [
        "### **Residual Connection과 Layer Normalization의 조합**\n",
        "\n",
        "Transformer는 **Residual Connection**과 **Layer Normalization**을 조합하여 다음과 같은 이점을 제공합니다:\n",
        "\n",
        "1. **정보 손실 방지**: Residual Connection이 입력 정보를 직접 전달.\n",
        "2. **학습 안정화**: Layer Normalization이 출력 값을 정규화.\n",
        "3. **효율적인 학습**: 두 메커니즘의 결합으로 모델이 빠르게 수렴."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82df9d7f",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My0yLuyekOyXsOyWtCDsspjrpqwg67Cc7KCE6rO87KCVKFNlcTJTZXHrtoDthLAgVHJhbnNmb3JtZXLquYzsp4ApXzIuaXB5bmI=",
          "cell_index": 28
        }
      },
      "source": [
        "### **Feed-Forward Neural Network (FFNN):**\n",
        "- Attention 층 다음에 오는 완전 연결 층으로 비선형성을 도입하여 모델의 표현력을 높입니다.\n",
        "- 역할: Attention 결과를 변환하여 더 높은 차원의 표현 학습.\n",
        "- 의도: 비선형 변환을 통한 복잡한 패턴 학습."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56edf327",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My0yLuyekOyXsOyWtCDsspjrpqwg67Cc7KCE6rO87KCVKFNlcTJTZXHrtoDthLAgVHJhbnNmb3JtZXLquYzsp4ApXzIuaXB5bmI=",
          "cell_index": 29
        }
      },
      "source": [
        "### **Output Layer:**\n",
        "- 역할: Transformer의 마지막 층으로, 예측을 생성.\n",
        "- 목적: 텍스트 생성, 분류 등 다양한 태스크 수행.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e1a775d",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My0yLuyekOyXsOyWtCDsspjrpqwg67Cc7KCE6rO87KCVKFNlcTJTZXHrtoDthLAgVHJhbnNmb3JtZXLquYzsp4ApXzIuaXB5bmI=",
          "cell_index": 30
        }
      },
      "source": [
        "## 4.Transformer의 주요 동작 단계\n",
        "\n",
        "Transformer는 입력 문장에서부터 최종 출력 결과를 생성하기까지 다음과 같은 과정을 거칩니다:\n",
        "\n",
        "1. **입력 문장의 토큰화 및 임베딩**  \n",
        "  - 입력 문장을 단어 또는 서브워드 단위로 토큰화합니다.  \n",
        "  - 각 토큰을 고차원 벡터로 변환하여 텍스트 데이터를 수치적으로 표현합니다.  \n",
        "\n",
        "    <img src=\"image/text_input.png\" width=\"400\">\n",
        "\n",
        "\n",
        "2. **포지셔널 인코딩 추가**  \n",
        "   - 각 토큰의 위치 정보를 포지셔널 인코딩 벡터로 표현하여 입력 임베딩에 더합니다.  \n",
        "   - 이는 모델이 입력 시퀀스의 순서를 이해할 수 있도록 돕습니다.\n",
        "\n",
        "   <img src=\"image/positional_encoding.png\" width=\"400\">\n",
        "\n",
        "3. **인코더에서 Self-Attention과 FFNN 적용**  \n",
        "   - Self-Attention 메커니즘을 통해 입력 시퀀스의 각 토큰이 다른 토큰과의 관계를 계산하여 문맥을 이해합니다.  \n",
        "   - Multi-Head Attention을 사용해 다양한 관점에서 관계를 분석합니다.  \n",
        "   - FFNN을 통해 비선형 변환을 수행하여 복잡한 패턴을 학습합니다.  \n",
        "   - Residual Connection과 Layer Normalization으로 안정성을 유지합니다.\n",
        "\n",
        "4. **디코더에서 마스크드 Self-Attention, 인코더-디코더 Attention, FFNN 적용**  \n",
        "   - 디코더는 이전 단계에서 생성된 단어를 기반으로 마스크드 Self-Attention을 수행하여 다음 단어를 예측합니다.  \n",
        "   - 인코더-디코더 Attention을 통해 인코더의 출력과 디코더의 입력 간의 관계를 모델링합니다.  \n",
        "   - FFNN으로 출력 특징을 변환하고 Residual Connection과 Layer Normalization으로 안정성을 유지합니다.\n",
        "\n",
        "5. **최종 출력 생성**  \n",
        "   - 디코더의 마지막 층에서 결과를 생성합니다.  \n",
        "   - 이를 통해 텍스트 생성, 번역, 요약 등 다양한 자연어 처리 태스크를 수행합니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58cc5395",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My0yLuyekOyXsOyWtCDsspjrpqwg67Cc7KCE6rO87KCVKFNlcTJTZXHrtoDthLAgVHJhbnNmb3JtZXLquYzsp4ApXzIuaXB5bmI=",
          "cell_index": 31
        }
      },
      "source": [
        "### 간단한 요약\n",
        "\n",
        "Transformer에서 **다음 단어를 예측할 때**는  \n",
        "- 인코더가 입력 시퀀스를 문맥 벡터들로 표현하고,  \n",
        "- 디코더가 이 인코더 출력 + 지금까지 생성된 단어를 함께 보면서 Self-Attention으로 다음 단어를 고른다고 볼 수 있습니다.\n",
        "\n",
        "핵심 처리 단계만 정리하면:\n",
        "\n",
        "1. **토큰화 + 임베딩 + 포지셔널 인코딩**으로 \"순서 있는 벡터 시퀀스\" 만들기  \n",
        "2. 인코더에서 **Self-Attention + FFNN + Residual/LayerNorm** 블록을 여러 번 쌓아 문맥 표현 학습  \n",
        "3. 디코더에서 **마스크드 Self-Attention + 인코더-디코더 Attention**으로 입력·출력 양쪽 문맥을 함께 참고  \n",
        "4. 마지막 선형층에서 **다음 단어 확률 분포** 출력  \n",
        "\n",
        "이 흐름 덕분에 Transformer는 긴 문맥과 복잡한 의존성을 잘 포착하면서도 병렬 처리가 가능합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2060dbbf",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My0yLuyekOyXsOyWtCDsspjrpqwg67Cc7KCE6rO87KCVKFNlcTJTZXHrtoDthLAgVHJhbnNmb3JtZXLquYzsp4ApXzIuaXB5bmI=",
          "cell_index": 32
        }
      },
      "source": [
        "## 5.Transformer의 의의와 영향\n",
        "- 성능 향상: 기계 번역, 텍스트 요약, 질문 답변 등에서 최고 성능을 달성.\n",
        "- 확장성: GPT, BERT와 같은 대규모 언어 모델의 기반.\n",
        "- 범용성: NLP를 넘어 컴퓨터 비전, 음성 인식 등 다양한 분야로 확장 가능.\n",
        "\n",
        "Transformer 모델은 그 혁신적인 구조와 뛰어난 성능으로 현대 NLP의 중심이 되었습니다.  \n",
        "병렬 처리 능력과 장거리 의존성 포착 능력은 이전 모델들의 한계를 극복했으며, 이는 더 큰 모델과 더 다양한 응용으로 이어지고 있습니다.  \n",
        "앞으로도 Transformer는 AI 발전의 핵심 요소로 계속 진화할 것으로 예상됩니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94c0ed8c",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My0yLuyekOyXsOyWtCDsspjrpqwg67Cc7KCE6rO87KCVKFNlcTJTZXHrtoDthLAgVHJhbnNmb3JtZXLquYzsp4ApXzIuaXB5bmI=",
          "cell_index": 33
        }
      },
      "source": [
        "## 6. 복습 문제"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bebf63c8",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My0yLuyekOyXsOyWtCDsspjrpqwg67Cc7KCE6rO87KCVKFNlcTJTZXHrtoDthLAgVHJhbnNmb3JtZXLquYzsp4ApXzIuaXB5bmI=",
          "cell_index": 34
        }
      },
      "source": [
        "#### 문제 1. Transformer는 왜 RNN을 완전히 대체할 수 있었는가?  \n",
        "Seq2Seq + Attention이 있었는데도 Transformer가 구조적으로 제공한 \"결정적 장점\"을 설명하세요.\n",
        "\n",
        "<details><summary>정답 보기</summary>\n",
        "\n",
        "→ Transformer는 RNN처럼 순차적으로 토큰을 처리할 필요가 없어 전체 시퀀스를 병렬로 연산할 수 있는데, 이 병렬성은 학습 속도를 획기적으로 높이고, self-attention이 장거리 의존성까지 안정적으로 포착해 성능까지 상승시키기 때문에 RNN 기반 구조를 대체할 수 있게 되었다.\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4e8c74e",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My0yLuyekOyXsOyWtCDsspjrpqwg67Cc7KCE6rO87KCVKFNlcTJTZXHrtoDthLAgVHJhbnNmb3JtZXLquYzsp4ApXzIuaXB5bmI=",
          "cell_index": 35
        }
      },
      "source": [
        "#### 문제 2. Multi-Head Attention이 단일 Attention보다 더 나은 이유  \n",
        "단순히 여러 관점을 본다가 아니라, **왜 여러 개로 나누는 것이 구조적으로 유리한지** 설명하세요.\n",
        "\n",
        "<details><summary>정답 보기</summary>\n",
        "\n",
        "→ 하나의 큰 attention 공간에서 모든 패턴을 동시에 학습하면 특정 방향으로 편향되기 쉬운데, 여러 head로 공간을 분리하면 각 head가 서로 다른 하위 표현(subspace)에 특화된 패턴을 독립적으로 학습해 정보 손실을 줄이고, 다양한 관계를 풍부하게 표현할 수 있기 때문이다.\n",
        "\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76145ef5",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My0yLuyekOyXsOyWtCDsspjrpqwg67Cc7KCE6rO87KCVKFNlcTJTZXHrtoDthLAgVHJhbnNmb3JtZXLquYzsp4ApXzIuaXB5bmI=",
          "cell_index": 36
        }
      },
      "source": [
        "#### 문제 3. Decoder에 Encoder-Decoder Attention이 없다면 어떤 문제가 생기는가?  \n",
        "입력 문장을 사용하지 못할 때 출력 품질이 어떻게 무너지는지 설명하세요.\n",
        "\n",
        "<details><summary>정답 보기</summary>\n",
        "\n",
        "→ Decoder가 입력 문장을 직접 참조할 수 없으면 출력이 입력 의미와 연결되지 않고 단순 언어 모델처럼 내부적으로만 일관된 문장을 생성해, 번역·요약처럼 입력 의미를 반영해야 하는 작업에서 심각한 의미 왜곡이 발생한다.\n",
        "\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93a74cb5",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My0yLuyekOyXsOyWtCDsspjrpqwg67Cc7KCE6rO87KCVKFNlcTJTZXHrtoDthLAgVHJhbnNmb3JtZXLquYzsp4ApXzIuaXB5bmI=",
          "cell_index": 37
        }
      },
      "source": [
        "#### 문제 4. Self-Attention이 O(n²)라 비싸지만 Transformer가 선택한 이유  \n",
        "단점보다 장점이 큰 이유를 논리적으로 설명하세요.\n",
        "\n",
        "<details><summary>정답 보기</summary>\n",
        "\n",
        "→ Self-attention은 모든 토큰 쌍의 상호 관계를 직접 계산하기 때문에 계산량은 많지만, 그 덕분에 RNN이 처리하지 못하는 장거리 의존성을 안정적으로 학습할 수 있고, 동시에 모든 토큰을 병렬 처리할 수 있어 전체 훈련 시간 측면에서는 오히려 효율적이기 때문이다.\n",
        "\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6835460c",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My0yLuyekOyXsOyWtCDsspjrpqwg67Cc7KCE6rO87KCVKFNlcTJTZXHrtoDthLAgVHJhbnNmb3JtZXLquYzsp4ApXzIuaXB5bmI=",
          "cell_index": 38
        }
      },
      "source": [
        "#### 문제 5. Attention 외에 FeedForward Network(FFN)도 필요한 이유 \n",
        "“비선형성 추가” 말고, 왜 구조적으로 FFN이 반드시 필요한가?\n",
        "\n",
        "<details><summary>정답 보기</summary>\n",
        "\n",
        "→ Attention은 관계를 조합하는 데 특화되어 있지만 각 토큰 자체의 표현을 깊게 변형시키는 능력은 제한적이므로, FFN이 토큰별로 비선형 변환을 추가함으로써 attention이 포착한 관계를 더 풍부한 고차원 표현으로 확장해 모델의 표현력을 크게 높여 준다.\n",
        "\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3ace347",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My0yLuyekOyXsOyWtCDsspjrpqwg67Cc7KCE6rO87KCVKFNlcTJTZXHrtoDthLAgVHJhbnNmb3JtZXLquYzsp4ApXzIuaXB5bmI=",
          "cell_index": 39
        }
      },
      "source": [
        "#### 문제 6. Residual Connection을 제거하면 학습이 어려워지는 이유  \n",
        "단순 \"gradient vanishing\"이 아니라 구조적으로 왜 발생하는지 설명하세요.\n",
        "\n",
        "<details><summary>정답 보기</summary>\n",
        "\n",
        "→ Residual connection이 없으면 각 층이 입력 전체 변환을 직접 학습해야 하므로,  \n",
        " 깊어질수록 함수 합성이 어려워지고 identity mapping조차 학습하기 힘들어진다.  \n",
        "Residual은 각 층이 입력에 대한 보정(residual)만 학습하도록 구조를 바꾸어,  \n",
        "깊은 네트워크에서도 최적화가 가능하게 한다.\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3229fcb",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My0yLuyekOyXsOyWtCDsspjrpqwg67Cc7KCE6rO87KCVKFNlcTJTZXHrtoDthLAgVHJhbnNmb3JtZXLquYzsp4ApXzIuaXB5bmI=",
          "cell_index": 40
        }
      },
      "source": [
        "#### 문제 7. Encoder에는 Masked Self-Attention이 필요 없는 이유  \n",
        "왜 Encoder는 미래 토큰을 가려야 할 필요가 없는지 논리적으로 설명하세요.\n",
        "\n",
        "<details><summary>정답 보기</summary>\n",
        "\n",
        "→ Encoder는 입력 전체를 한 번에 해석해야 입력 문장의 양방향 문맥을 정확히 파악할 수 있으므로, 미래 토큰을 가리면 문장의 전체 의미 구조를 훼손하게 되어 모델의 인식 능력이 떨어지기 때문이다.\n",
        "\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02f677ca",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My0yLuyekOyXsOyWtCDsspjrpqwg67Cc7KCE6rO87KCVKFNlcTJTZXHrtoDthLAgVHJhbnNmb3JtZXLquYzsp4ApXzIuaXB5bmI=",
          "cell_index": 41
        }
      },
      "source": [
        "#### 문제 8. Transformer가 Seq2Seq + Attention 대비 혁신적이었던 이유  \n",
        "단순 구조 변화가 아니라 “왜 혁신이었는지” 설명하세요.\n",
        "\n",
        "<details><summary>정답 보기</summary>\n",
        "\n",
        "→ 기존 Seq2Seq는 RNN 구조적 특성 때문에 토큰을 순서대로 처리해야 해 병렬화가 불가능했지만, Transformer는 self-attention 기반으로 전체 시퀀스를 한 번에 처리하며 순차 의존성을 제거해 학습 속도와 성능을 동시에 크게 개선했기 때문에 혁신적이었다.\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de135555",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My0yLuyekOyXsOyWtCDsspjrpqwg67Cc7KCE6rO87KCVKFNlcTJTZXHrtoDthLAgVHJhbnNmb3JtZXLquYzsp4ApXzIuaXB5bmI=",
          "cell_index": 42
        }
      },
      "source": [
        "#### 문제 9. Attention Score가 Softmax를 사용하는 이유  \n",
        "왜 Sigmoid나 ReLU가 아닌 Softmax인지 개념적으로 설명하세요.\n",
        "\n",
        "<details><summary>정답 보기</summary>\n",
        "\n",
        "→ Self-attention은 여러 위치 중 어떤 위치에 “상대적으로 더 집중할지”를 결정해야 하는데 softmax는 전체 토큰 점수를 하나의 확률 분포로 변환하여 각 위치의 중요도를 자연스럽게 비교 가능하게 만들기 때문에 attention 가중치로 이상적이다.\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "852688ff",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My0yLuyekOyXsOyWtCDsspjrpqwg67Cc7KCE6rO87KCVKFNlcTJTZXHrtoDthLAgVHJhbnNmb3JtZXLquYzsp4ApXzIuaXB5bmI=",
          "cell_index": 43
        }
      },
      "source": [
        "## 7. Transformer 기계 번역 실습 (영 -> 한)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "3d377938",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My0yLuyekOyXsOyWtCDsspjrpqwg67Cc7KCE6rO87KCVKFNlcTJTZXHrtoDthLAgVHJhbnNmb3JtZXLquYzsp4ApXzIuaXB5bmI=",
          "cell_index": 44
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "edbbab3869d845608698a4e2150b1609",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/950 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# 1) 병렬 말뭉치 로드: 영어(en) ↔ 한국어(ko) 쌍이 들어 있음\n",
        "dataset = load_dataset(\"strongminsu/ko-en-structured-translations\")\n",
        "\n",
        "# 2) 한국어-영어 양방향 ELECTRA 토크나이저 로드\n",
        "#    - CLS 토큰(id=2)이 “시작” 역할을 할 것\n",
        "#    - SEP 토큰(id=3)이 “문장 끝(EOS)” 역할을 할 것\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"tunib/electra-ko-en-small\")\n",
        "\n",
        "# 학습/평가 셔플\n",
        "dataset_train = dataset[\"train\"].shuffle(seed=42)\n",
        "dataset_test = dataset[\"test\"].shuffle(seed=42)\n",
        "\n",
        "max_length = 64 # 문장을 토큰 64개 길이로 자르고/패딩\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    # text/text_target를 동시에 넘기면 HF가 알아서 labels를 만들어 줌\n",
        "    # padding=\"max_length\"로 길이를 맞추면 DataLoader가 바로 텐서 배치를 만들 수 있음\n",
        "    model_inputs = tokenizer(\n",
        "        text=examples[\"en\"],       # 인코더 입력: 영어\n",
        "        text_target=examples[\"ko\"],# 디코더 타깃: 한국어\n",
        "        truncation=True,           # max_length 넘으면 잘라서 메모리/학습 안정\n",
        "        padding=\"max_length\",      # 길이를 고정 → batch 텐서화 용이\n",
        "        max_length=max_length,\n",
        "    )\n",
        "    return model_inputs\n",
        "\n",
        "# 4) 토크나이징 + PyTorch 텐서 포맷 지정 (input_ids/labels만 남김)\n",
        "tokenized_dataset_train = dataset_train.map(preprocess_function, batched=True)\n",
        "tokenized_dataset_train.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "\n",
        "tokenized_dataset_test = dataset_test.map(preprocess_function, batched=True)\n",
        "tokenized_dataset_test.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fdc5b4d9",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My0yLuyekOyXsOyWtCDsspjrpqwg67Cc7KCE6rO87KCVKFNlcTJTZXHrtoDthLAgVHJhbnNmb3JtZXLquYzsp4ApXzIuaXB5bmI=",
          "cell_index": 45
        }
      },
      "source": [
        "Transformer 모델 로드 & 학습 (Hugging Face Trainer 사용)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "51dc36d2",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My0yLuyekOyXsOyWtCDsspjrpqwg67Cc7KCE6rO87KCVKFNlcTJTZXHrtoDthLAgVHJhbnNmb3JtZXLquYzsp4ApXzIuaXB5bmI=",
          "cell_index": 46
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(tokenized_dataset_train, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(tokenized_dataset_test, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# 특수 토큰 ID: 패딩/시작/끝\n",
        "pad_id = tokenizer.pad_token_id                     # 손실 계산에서 무시할 패딩\n",
        "BOS_TOKEN_ID = tokenizer.cls_token_id # 디코더 시작 토큰\n",
        "EOS_TOKEN_ID = tokenizer.sep_token_id # 디코더 종료 토큰\n",
        "vocab_size = tokenizer.vocab_size     # 임베딩/출력 차원"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "4e82a539",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My0yLuyekOyXsOyWtCDsspjrpqwg67Cc7KCE6rO87KCVKFNlcTJTZXHrtoDthLAgVHJhbnNmb3JtZXLquYzsp4ApXzIuaXB5bmI=",
          "cell_index": 47
        }
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch.nn as nn\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer는 토큰 순서를 모름 → sin/cos 파형으로 위치 정보를 임베딩에 더해 준다.\n",
        "    입력/출력 shape: (seq_len, batch, d_model)\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, max_len=512):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, d_model, 2, dtype=torch.float32) * (-math.log(10000.0) / d_model)\n",
        "        )\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(1)  # (max_len, 1, d_model)\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 입력 임베딩에 앞쪽 seq_len만큼의 위치벡터를 더함\n",
        "        seq_len = x.size(0)\n",
        "        return x + self.pe[:seq_len]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "8c0fbb7c",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My0yLuyekOyXsOyWtCDsspjrpqwg67Cc7KCE6rO87KCVKFNlcTJTZXHrtoDthLAgVHJhbnNmb3JtZXLquYzsp4ApXzIuaXB5bmI=",
          "cell_index": 48
        }
      },
      "outputs": [],
      "source": [
        "class TransformerMTModel(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=256, nhead=4, num_encoder_layers=2,\n",
        "                 num_decoder_layers=2, dim_feedforward=512, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        \n",
        "        # 1) 소스/타깃 임베딩 테이블\n",
        "        self.src_embed = nn.Embedding(vocab_size, d_model)\n",
        "        self.tgt_embed = nn.Embedding(vocab_size, d_model)\n",
        "        \n",
        "        # 2) 위치 인코딩(인코더/디코더용)\n",
        "        self.pos_encoder = PositionalEncoding(d_model)\n",
        "        self.pos_decoder = PositionalEncoding(d_model)\n",
        "\n",
        "        # 표준 PyTorch Transformer: batch_first=False → (seq_len, batch, dim)\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            num_encoder_layers=num_encoder_layers,\n",
        "            num_decoder_layers=num_decoder_layers,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "            batch_first=False  # (seq_len, batch, d_model)\n",
        "        )\n",
        "        # 4) 디코더 출력 → 단어 분포로 투사\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def _generate_square_subsequent_mask(self, sz):\n",
        "        # 디코더가 미래 토큰을 못 보도록 상삼각 -inf 마스크 생성\n",
        "        mask = torch.triu(torch.ones(sz, sz, device=self.fc_out.weight.device) == 1).transpose(0, 1)\n",
        "        mask = mask.float().masked_fill(mask == 0, float(\"-inf\")).masked_fill(mask == 1, 0.0)\n",
        "        return mask\n",
        "\n",
        "    def forward(self, src, tgt, src_key_padding_mask=None, tgt_key_padding_mask=None):\n",
        "        \"\"\"\n",
        "        src, tgt: (batch, seq_len)\n",
        "        1) (seq_len, batch)로 전치 → Transformer가 기대하는 입력 모양\n",
        "        2) 임베딩 + 위치인코딩\n",
        "        3) 인코더/디코더 실행 + 패딩 마스크 전달\n",
        "        4) 단어 분포(logits) 반환\n",
        "        \"\"\"\n",
        "        src = src.transpose(0, 1)  # (seq_len, batch)\n",
        "        tgt = tgt.transpose(0, 1)  # (seq_len, batch)\n",
        "        \n",
        "        # 임베딩 스케일링 + 위치인코딩\n",
        "        src_emb = self.pos_encoder(self.src_embed(src) * math.sqrt(self.d_model))\n",
        "        tgt_emb = self.pos_decoder(self.tgt_embed(tgt) * math.sqrt(self.d_model))\n",
        "\n",
        "        # 디코더용 미래 차단 마스크\n",
        "        tgt_mask = self._generate_square_subsequent_mask(tgt.size(0))\n",
        "\n",
        "        # 인코더-디코더 패딩 마스크 전달\n",
        "        memory = self.transformer.encoder(\n",
        "            src_emb,\n",
        "            src_key_padding_mask=src_key_padding_mask\n",
        "        )\n",
        "        output = self.transformer.decoder(\n",
        "            tgt_emb,\n",
        "            memory,\n",
        "            tgt_mask=tgt_mask,\n",
        "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
        "            memory_key_padding_mask=src_key_padding_mask\n",
        "        )\n",
        "        logits = self.fc_out(output)  # (seq_len, batch, vocab)\n",
        "        return logits.transpose(0, 1)  # 다시 (batch, seq_len, vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "e2b24c90",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My0yLuyekOyXsOyWtCDsspjrpqwg67Cc7KCE6rO87KCVKFNlcTJTZXHrtoDthLAgVHJhbnNmb3JtZXLquYzsp4ApXzIuaXB5bmI=",
          "cell_index": 49
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/root/micromamba/envs/py310/lib/python3.10/site-packages/torch/nn/modules/transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n",
            "/root/micromamba/envs/py310/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20 | Train Loss: 5.3209\n",
            "Epoch 2/20 | Train Loss: 3.7625\n",
            "Epoch 3/20 | Train Loss: 3.1421\n",
            "Epoch 4/20 | Train Loss: 2.6899\n",
            "Epoch 5/20 | Train Loss: 2.3195\n",
            "Epoch 6/20 | Train Loss: 2.0133\n",
            "Epoch 7/20 | Train Loss: 1.7666\n",
            "Epoch 8/20 | Train Loss: 1.5272\n",
            "Epoch 9/20 | Train Loss: 1.3581\n",
            "Epoch 10/20 | Train Loss: 1.1769\n",
            "Epoch 11/20 | Train Loss: 1.0287\n",
            "Epoch 12/20 | Train Loss: 0.9111\n",
            "Epoch 13/20 | Train Loss: 0.8251\n",
            "Epoch 14/20 | Train Loss: 0.7285\n",
            "Epoch 15/20 | Train Loss: 0.6505\n",
            "Epoch 16/20 | Train Loss: 0.6010\n",
            "Epoch 17/20 | Train Loss: 0.5408\n",
            "Epoch 18/20 | Train Loss: 0.4854\n",
            "Epoch 19/20 | Train Loss: 0.4624\n",
            "Epoch 20/20 | Train Loss: 0.4297\n"
          ]
        }
      ],
      "source": [
        "model = TransformerMTModel(vocab_size=vocab_size, d_model=256, nhead=4,\n",
        "                           num_encoder_layers=2, num_decoder_layers=2).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_id) # PAD 토큰은 손실에서 제외\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
        "\n",
        "def make_padding_mask(batch_ids, pad_id):\n",
        "    # PAD 위치를 True로 표시 → Transformer가 어텐션할 때 무시\n",
        "    return (batch_ids == pad_id)\n",
        "\n",
        "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
        "    \"\"\"\n",
        "    - 디코더 입력은 정답을 한 칸 오른쪽으로 민 시퀀스([BOS, y0, y1, ...]).\n",
        "    - 출력 logits과 원래 라벨을 같은 위치에 맞춰서 CE 손실 계산.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for batch in dataloader:\n",
        "        src = batch[\"input_ids\"].to(device)   # (batch, seq_len)\n",
        "        trg = batch[\"labels\"].to(device)      # (batch, seq_len)\n",
        "\n",
        "        # 디코더 입력 만들기: 라벨의 첫 토큰(BOS)을 앞에 두고 오른쪽으로 시프트\n",
        "        tgt_input = torch.full_like(trg, pad_id)\n",
        "        #tgt_input[:, 0] = trg[:, 0]       # 이미 BOS가 들어 있음\n",
        "        tgt_input[:, 0] = BOS_TOKEN_ID\n",
        "        tgt_input[:, 1:] = trg[:, :-1]\n",
        "\n",
        "        # 패딩 마스크: PAD 위치 True → 어텐션에서 무시\n",
        "        src_key_padding_mask = make_padding_mask(src, pad_id)   # (batch, seq_len)\n",
        "        tgt_key_padding_mask = make_padding_mask(tgt_input, pad_id)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(src, tgt_input,\n",
        "                       src_key_padding_mask=src_key_padding_mask,\n",
        "                       tgt_key_padding_mask=tgt_key_padding_mask)  # (batch, seq_len, vocab)\n",
        "\n",
        "        # (batch, seq_len, vocab) → (batch*seq_len, vocab)로 납작하게 펴서 CE 계산\n",
        "        vocab_dim = logits.size(-1)\n",
        "        #loss = criterion(logits.reshape(-1, vocab_dim),trg.reshape(-1))\n",
        "        loss = criterion(logits[:, 1:, :].reshape(-1, vocab_dim), trg[:, 1:].reshape(-1))\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # 기울기 폭주 방지\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "# 학습 실행\n",
        "N_EPOCHS = 20 \n",
        "train_losses = []\n",
        "for epoch in range(N_EPOCHS):\n",
        "    loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
        "    train_losses.append(loss)\n",
        "    print(f\"Epoch {epoch+1}/{N_EPOCHS} | Train Loss: {loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "7fe6d2c5",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My0yLuyekOyXsOyWtCDsspjrpqwg67Cc7KCE6rO87KCVKFNlcTJTZXHrtoDthLAgVHJhbnNmb3JtZXLquYzsp4ApXzIuaXB5bmI=",
          "cell_index": 50
        }
      },
      "outputs": [],
      "source": [
        "def translate_with_transformer(model, tokenizer, sentence, max_len=64):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        ## 1) 입력 문장 토크나이즈 (배치=1)\n",
        "        enc = tokenizer(sentence, truncation=True, max_length=max_len)\n",
        "        # enc[\"input_ids\"]: [tok1, tok2, ...]\n",
        "        src_ids = torch.tensor([enc[\"input_ids\"]], dtype=torch.long, device=device)  # (1, src_len)\n",
        "        src_key_padding_mask = make_padding_mask(src_ids, pad_id)\n",
        "\n",
        "        # 2) 인코더 통과\n",
        "        src = src_ids.transpose(0, 1)  # (src_len, 1)\n",
        "        src_emb = model.pos_encoder(model.src_embed(src) * math.sqrt(model.d_model))\n",
        "        memory = model.transformer.encoder(\n",
        "            src_emb,\n",
        "            src_key_padding_mask=src_key_padding_mask\n",
        "        )\n",
        "\n",
        "        # 3) 디코더 초기 입력: BOS\n",
        "        ys = torch.tensor([[tokenizer.cls_token_id]], device=device)  # (1, 1)\n",
        "        \n",
        "        # 4) 토큰 하나씩 생성 (greedy)\n",
        "        for _ in range(max_len - 1):\n",
        "            tgt = ys.transpose(0, 1)  # (seq_len, 1)\n",
        "            tgt_emb = model.pos_decoder(model.tgt_embed(tgt) * math.sqrt(model.d_model))\n",
        "            tgt_mask = model._generate_square_subsequent_mask(tgt.size(0)) # 미래 차단\n",
        "\n",
        "            out = model.transformer.decoder(\n",
        "                tgt_emb,\n",
        "                memory,\n",
        "                tgt_mask=tgt_mask,\n",
        "                memory_key_padding_mask=src_key_padding_mask\n",
        "            )\n",
        "            logits = model.fc_out(out[-1, 0, :])  #  마지막 시점 로짓만 사용\n",
        "            next_token = logits.argmax(-1).unsqueeze(0).unsqueeze(0)  # (1,1)\n",
        "            ys = torch.cat([ys, next_token], dim=1)  # 생성 토큰을 이어붙임\n",
        "\n",
        "            # EOS(SEP)가 나오면 종료\n",
        "            if next_token.item() == tokenizer.sep_token_id: # 해당 토크나이저에서는 종료토큰 sep_token_id\n",
        "                break\n",
        "\n",
        "        pred_ids = ys[0].tolist()\n",
        "        return tokenizer.decode(pred_ids, skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "id": "2189188b",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My0yLuyekOyXsOyWtCDsspjrpqwg67Cc7KCE6rO87KCVKFNlcTJTZXHrtoDthLAgVHJhbnNmb3JtZXLquYzsp4ApXzIuaXB5bmI=",
          "cell_index": 51
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "영어 원문 : Various exhibitions are held at the art museums in Korea.\n",
            "\n",
            "정답 번역 : 한국의 미술관에서는 다양한 전시가 열리고 있어요.\n",
            "\n",
            "모델 번역: 한국의 미술관에서는 다양한 예술 형식에서 한국에서 한국에서의 미술관이 풍부해요.\n"
          ]
        }
      ],
      "source": [
        "# 예시 번역\n",
        "sample = next(iter(train_loader))\n",
        "\n",
        "# 1) 소스 문장 (영어) 텐서 준비\n",
        "src_sample = sample['input_ids'][0].unsqueeze(0).to(device)  # shape: (1, seq_len)\n",
        "\n",
        "# 1) 정답 문장 (영어) 텐서 준비\n",
        "ans_sample = sample['labels'][0].unsqueeze(0).to(device)  # shape: (1, seq_len)\n",
        "\n",
        "\n",
        "# 토큰 → 문자열로 복원해 비교\n",
        "src_text = tokenizer.decode(src_sample[0].tolist(), skip_special_tokens=True)\n",
        "ref_text = tokenizer.decode(ans_sample[0].tolist(), skip_special_tokens=True)\n",
        "print(\"영어 원문 :\", src_text)\n",
        "print(\"\\n정답 번역 :\", ref_text)\n",
        "print(\"\\n모델 번역:\", translate_with_transformer(model, tokenizer, src_text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "id": "204ffd7f",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My0yLuyekOyXsOyWtCDsspjrpqwg67Cc7KCE6rO87KCVKFNlcTJTZXHrtoDthLAgVHJhbnNmb3JtZXLquYzsp4ApXzIuaXB5bmI=",
          "cell_index": 52
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "영어 원문 : I found a new café.\n",
            "\n",
            "정답 번역 : 새로운 카페 발견했어.\n",
            "\n",
            "모델 번역: 새로운 카페를 발견했어.\n"
          ]
        }
      ],
      "source": [
        "# 예시 번역\n",
        "sample = next(iter(test_loader))\n",
        "\n",
        "# 1) 소스 문장 (영어) 텐서 준비\n",
        "src_sample = sample['input_ids'][0].unsqueeze(0).to(device)  # shape: (1, seq_len)\n",
        "\n",
        "# 1) 정답 문장 (영어) 텐서 준비\n",
        "ans_sample = sample['labels'][0].unsqueeze(0).to(device)  # shape: (1, seq_len)\n",
        "\n",
        "\n",
        "# 토큰 → 문자열로 복원해 비교\n",
        "src_text = tokenizer.decode(src_sample[0].tolist(), skip_special_tokens=True)\n",
        "ref_text = tokenizer.decode(ans_sample[0].tolist(), skip_special_tokens=True)\n",
        "print(\"영어 원문 :\", src_text)\n",
        "print(\"\\n정답 번역 :\", ref_text)\n",
        "print(\"\\n모델 번역:\", translate_with_transformer(model, tokenizer, src_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbfceae2",
      "metadata": {
        "watermark": {
          "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
          "inserted_date": "2026-01-19",
          "filename": "My0yLuyekOyXsOyWtCDsspjrpqwg67Cc7KCE6rO87KCVKFNlcTJTZXHrtoDthLAgVHJhbnNmb3JtZXLquYzsp4ApXzIuaXB5bmI=",
          "cell_index": 53
        }
      },
      "source": [
        "다음 노트에서는 Transformer를 기반으로 한 대표 사전학습 모델 BERT/GPT를 자세히 살펴봅니다"
      ]
    }
  ],
  "metadata": {
    "encoded_email": [
      "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ=="
    ],
    "filename": "My0yLuyekOyXsOyWtCDsspjrpqwg67Cc7KCE6rO87KCVKFNlcTJTZXHrtoDthLAgVHJhbnNmb3JtZXLquYzsp4ApLmlweW5i",
    "inserted_date": [
      "2025-09-23"
    ],
    "kernelspec": {
      "display_name": "py310",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    },
    "watermark": {
      "encoded_email": "cmxhYWxzdG4xNTA0QG5hdmVyLmNvbQ==",
      "inserted_date": "2026-01-19",
      "filename": "My0yLuyekOyXsOyWtCDsspjrpqwg67Cc7KCE6rO87KCVKFNlcTJTZXHrtoDthLAgVHJhbnNmb3JtZXLquYzsp4ApXzIuaXB5bmI="
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}